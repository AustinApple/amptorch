Fri Mar 13 06:39:39 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [0.972, 6.379], 'O': [1.09, 8.575], 'Cu': [2.168, 3.8386]}
Optimizer results: 
 fun: 0.48972314458295774 
 message: Local minimum reached (|pg| ~= 0) 
 nfev: 26 
 nit: 7 
 success: True
Fitted LJ parameters: {'C': array([1.e+00, 1.e-05]), 'O': array([1.e+00, 1.e-05]), 'Cu': array([1.e+00, 1.e-05])} 

a: 15.0
Optimization time: 398.3099944591522 

Filename: COCu_rep_new_200_5k_LJ_100_iter_3
Dataset size: 500
Target scaling: [8.841315589518457, 6.990871105586963e-07]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 500
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.0071       0.3135       5.4952       0.1992 51.5919
    2       0.0104       0.3131       4.9054       0.2015 47.8758
    3       0.0077       0.3197       4.0676       0.2074 47.7026
    4       0.0100       0.3171       3.5171       0.2061 49.6183
    5       0.0082       0.3291       3.2196       0.2200 50.1234
    6       0.0057       0.3196       2.9720       0.2059 47.8091
    7       0.0107       0.3026       2.6837       0.1888 47.8249
    8       0.0129       0.2968       2.1788       0.1845 50.1227
    9       0.0118       0.3012       1.8786       0.1885 50.2276
   10       0.0099       0.2812       1.4817       0.1631 48.3255
   11       0.0063       0.2729       1.2875       0.1510 47.9329
   12       0.0109       0.2766       1.1560       0.1589 47.8514
   13       0.0078       0.2676       0.9629       0.1463 47.6427
   14       0.0072       0.2668       0.8198       0.1450 48.2742
   15       0.0038       0.2677       0.7381       0.1441 49.6475
   16       0.0061       0.2637       0.6756       0.1409 50.1826
   17       0.0080       0.2608       0.6211       0.1392 49.7340
   18       0.0083       0.2597       0.5822       0.1384 47.6221
   19       0.0080       0.2577       0.5530       0.1361 47.8143
   20       0.0076       0.2586       0.5262       0.1367 48.0900
   21       0.0072       0.2598       0.5012       0.1376 48.2081
   22       0.0081       0.2590       0.4782       0.1374 47.7747
   23       0.0088       0.2576       0.4597       0.1366 49.5252
   24       0.0096       0.2550       0.4357       0.1347 48.2722
   25       0.0087       0.2534       0.4234       0.1322 49.4623
   26       0.0049       0.2504       0.4081       0.1265 49.6245
   27       0.0051       0.2472       0.3887       0.1235 47.7791
   28       0.0060       0.2451       0.3716       0.1220 47.8006
   29       0.0042       0.2416       0.3548       0.1176 47.8444
   30       0.0035       0.2395       0.3379       0.1154 47.6792
   31       0.0046       0.2380       0.3286       0.1143 49.6251
   32       0.0056       0.2342       0.3188       0.1113 58.7504
   33       0.0061       0.2312       0.3070       0.1087 48.5705
   34       0.0057       0.2289       0.3010       0.1064 49.8038
   35       0.0042       0.2285       0.2921       0.1053 50.8690
   36       0.0034       0.2270       0.2874       0.1037 50.8486
   37       0.0033       0.2249       0.2806       0.1017 52.4925
   38       0.0036       0.2235       0.2753       0.1006 50.3380
   39       0.0046       0.2215       0.2716       0.0992 52.3918
   40       0.0045       0.2205       0.2649       0.0982 52.3931
   41       0.0056       0.2193       0.2596       0.0978 50.4080
   42       0.0052       0.2169       0.2562       0.0954 52.2884
   43       0.0041       0.2175       0.2496       0.0954 50.4097
   44       0.0036       0.2159       0.2454       0.0939 50.7170
   45       0.0042       0.2113       0.2342       0.0902 50.5657
   46       0.0035       0.2090       0.2266       0.0880 50.5146
   47       0.0041       0.2068       0.2226       0.0864 52.4533
   48       0.0033       0.2059       0.2190       0.0853 52.0756
   49       0.0030       0.2041       0.2136       0.0838 50.9138
   50       0.0027       0.1981       0.2091       0.0788 52.8839
   51       0.0026       0.1856       0.2019       0.0693 50.5363
   52       0.0027       0.1673       0.1949       0.0563 50.5016
   53       0.0024       0.1540       0.1854       0.0477 52.3706
   54       0.0026       0.1456       0.1760       0.0427 52.3331
   55       0.0024       0.1382       0.1724       0.0385 50.4256
   56       0.0026       0.1304       0.1672       0.0343 50.5299
   57       0.0026       0.1275       0.1620       0.0329 52.7754
   58       0.0031       0.1264       0.1572       0.0324 52.1453
   59       0.0023       0.1215       0.1535       0.0298 52.9935
   60       0.0022       0.1210       0.1499       0.0295 52.5461
   61       0.0020       0.1230       0.1478       0.0305 52.1216
   62       0.0021       0.1222       0.1465       0.0301 50.2849
   63       0.0022       0.1219       0.1443       0.0300 52.2953
   64       0.0023       0.1244       0.1423       0.0312 50.0077
   65       0.0026       0.1189       0.1408       0.0286 50.3886
   66       0.0023       0.1210       0.1387       0.0295 52.3035
   67       0.0024       0.1164       0.1367       0.0274 50.4387
   68       0.0025       0.1156       0.1345       0.0271 50.3758
   69       0.0020       0.1112       0.1327       0.0250 50.0763
   70       0.0023       0.1103       0.1301       0.0246 52.4997
   71       0.0023       0.1110       0.1280       0.0249 52.4593
   72       0.0030       0.1132       0.1262       0.0261 51.0371
   73       0.0025       0.1112       0.1246       0.0250 52.4196
   74       0.0024       0.1097       0.1224       0.0243 52.8274
   75       0.0020       0.1107       0.1207       0.0247 50.1203
   76       0.0026       0.1124       0.1190       0.0256 52.4887
   77       0.0025       0.1122       0.1172       0.0255 50.7426
   78       0.0025       0.1112       0.1162       0.0250 50.4111
   79       0.0023       0.1122       0.1150       0.0254 50.4181
   80       0.0018       0.1105       0.1141       0.0246 52.3307
   81       0.0018       0.1078       0.1124       0.0234 50.3903
   82       0.0020       0.1072       0.1113       0.0232 52.6804
   83       0.0021       0.1050       0.1100       0.0223 50.2759
   84       0.0021       0.1049       0.1089       0.0222 51.9237
   85       0.0019       0.1031       0.1081       0.0215 50.2507
   86       0.0019       0.1035       0.1062       0.0216 50.6139
   87       0.0018       0.1022       0.1052       0.0210 49.8007
   88       0.0016       0.1019       0.1044       0.0209 50.6182
   89       0.0017       0.1006       0.1029       0.0204 50.2124
   90       0.0017       0.1010       0.1021       0.0206 49.9033
   91       0.0016       0.0977       0.1015       0.0192 49.9977
   92       0.0015       0.0959       0.1002       0.0185 52.4557
   93       0.0019       0.0938       0.0989       0.0178 52.6061
   94       0.0015       0.0912       0.0964       0.0168 52.2957
   95       0.0015       0.0903       0.0953       0.0164 50.3975
   96       0.0015       0.0907       0.0945       0.0166 52.1215
   97       0.0014       0.0878       0.0937       0.0155 52.1198
   98       0.0015       0.0861       0.0917       0.0149 50.1546
   99       0.0014       0.0842       0.0907       0.0143 50.0132
  100       0.0015       0.0833       0.0895       0.0140 50.1876
  101       0.0014       0.0842       0.0882       0.0143 50.6773
  102       0.0015       0.0827       0.0876       0.0138 50.1998
  103       0.0016       0.0819       0.0869       0.0135 51.8967
  104       0.0016       0.0818       0.0858       0.0135 50.7676
  105       0.0015       0.0812       0.0849       0.0133 52.7599
  106       0.0015       0.0813       0.0835       0.0133 51.8235
  107       0.0014       0.0810       0.0826       0.0132 52.1364
  108       0.0015       0.0795       0.0816       0.0128 50.0486
  109       0.0021       0.0765       0.0809       0.0119 50.0314
  110       0.0019       0.0760       0.0794       0.0117 50.5242
  111       0.0017       0.0762       0.0787       0.0118 50.0325
  112       0.0020       0.0752       0.0779       0.0115 52.0028
  113       0.0015       0.0769       0.0761       0.0119 49.9995
  114       0.0015       0.0754       0.0752       0.0115 51.9931
  115       0.0016       0.0741       0.0741       0.0111 50.6239
  116       0.0018       0.0736       0.0733       0.0110 50.0231
  117       0.0017       0.0739       0.0726       0.0111 50.0678
  118       0.0016       0.0737       0.0718       0.0110 52.0781
  119       0.0018       0.0724       0.0710       0.0107 50.6126
  120       0.0018       0.0703       0.0700       0.0100 50.1779
  121       0.0017       0.0704       0.0693       0.0101 52.7071
  122       0.0017       0.0693       0.0685       0.0098 50.0410
  123       0.0018       0.0699       0.0680       0.0099 52.1630
  124       0.0018       0.0708       0.0671       0.0102 52.6516
  125       0.0018       0.0700       0.0664       0.0099 52.4536
  126       0.0022       0.0708       0.0658       0.0103 50.7791
  127       0.0022       0.0693       0.0650       0.0098 50.2105
  128       0.0020       0.0702       0.0644       0.0100 50.7360
  129       0.0021       0.0686       0.0633       0.0096 49.9229
  130       0.0023       0.0686       0.0625       0.0097 50.1654
  131       0.0019       0.0681       0.0616       0.0094 52.0393
  132       0.0016       0.0686       0.0609       0.0096 49.9278
  133       0.0017       0.0676       0.0603       0.0093 50.2630
  134       0.0016       0.0681       0.0597       0.0094 52.1749
  135       0.0018       0.0667       0.0589       0.0091 51.9738
  136       0.0020       0.0674       0.0585       0.0093 51.9879
  137       0.0022       0.0680       0.0579       0.0095 52.0416
  138       0.0024       0.0685       0.0573       0.0097 52.7811
  139       0.0022       0.0694       0.0570       0.0099 50.6805
  140       0.0019       0.0705       0.0564       0.0101 51.0681
  141       0.0019       0.0716       0.0556       0.0104 50.3604
  142       0.0015       0.0732       0.0550       0.0108 50.3169
  143       0.0014       0.0723       0.0544       0.0106 52.5919
  144       0.0014       0.0700       0.0538       0.0099 50.7842
  145       0.0014       0.0700       0.0535       0.0099 50.9281
  146       0.0015       0.0694       0.0531       0.0097 52.7092
  147       0.0014       0.0680       0.0528       0.0093 50.5304
  148       0.0014       0.0665       0.0523       0.0089 52.8281
  149       0.0015       0.0654       0.0521       0.0087 52.8682
  150       0.0018       0.0640       0.0517       0.0084 52.8108
  151       0.0016       0.0645       0.0514       0.0084 52.6357
  152       0.0018       0.0645       0.0511       0.0085 50.6471
  153       0.0018       0.0635       0.0508       0.0082 50.3787
  154       0.0017       0.0653       0.0505       0.0087 50.1632
  155       0.0019       0.0660       0.0503       0.0089 50.3305
  156       0.0022       0.0662       0.0499       0.0090 50.6128
  157       0.0024       0.0662       0.0497       0.0091 49.9644
  158       0.0021       0.0671       0.0493       0.0092 50.1257
  159       0.0022       0.0666       0.0491       0.0091 52.1134
  160       0.0022       0.0665       0.0487       0.0091 50.1193
  161       0.0022       0.0669       0.0484       0.0092 50.3299
  162       0.0019       0.0670       0.0482       0.0092 50.1263
  163       0.0018       0.0682       0.0478       0.0095 51.9558
  164       0.0017       0.0686       0.0475       0.0095 49.9140
  165       0.0018       0.0696       0.0474       0.0099 49.9943
  166       0.0018       0.0698       0.0472       0.0099 51.9735
  167       0.0017       0.0689       0.0470       0.0096 49.8772
  168       0.0017       0.0700       0.0468       0.0100 50.3430
  169       0.0018       0.0690       0.0466       0.0097 50.0459
  170       0.0018       0.0710       0.0465       0.0102 51.8703
  171       0.0019       0.0681       0.0463       0.0094 50.1559
  172       0.0019       0.0683       0.0461       0.0095 50.0575
  173       0.0022       0.0689       0.0460       0.0097 50.0997
  174       0.0024       0.0693       0.0457       0.0099 49.9251
  175       0.0024       0.0693       0.0455       0.0099 49.9421
  176       0.0024       0.0688       0.0453       0.0097 52.0597
  177       0.0024       0.0695       0.0452       0.0100 52.0862
  178       0.0024       0.0697       0.0451       0.0100 50.0121
  179       0.0022       0.0702       0.0450       0.0101 52.5509
  180       0.0026       0.0685       0.0448       0.0097 51.9472
  181       0.0026       0.0699       0.0445       0.0101 50.1471
  182       0.0024       0.0707       0.0443       0.0103 50.0390
  183       0.0024       0.0694       0.0442       0.0099 51.9882
  184       0.0023       0.0704       0.0440       0.0102 51.9553
  185       0.0022       0.0698       0.0439       0.0100 49.8877
  186       0.0023       0.0693       0.0437       0.0099 48.7929
  187       0.0022       0.0697       0.0435       0.0100 50.4095
  188       0.0022       0.0715       0.0434       0.0105 48.4835
  189       0.0022       0.0719       0.0432       0.0106 48.3339
  190       0.0021       0.0704       0.0430       0.0101 48.4827
  191       0.0020       0.0690       0.0428       0.0097 50.0818
  192       0.0021       0.0687       0.0427       0.0097 49.8423
  193       0.0022       0.0672       0.0425       0.0093 48.6297
  194       0.0022       0.0681       0.0423       0.0095 49.9831
  195       0.0023       0.0665       0.0421       0.0091 49.9062
  196       0.0023       0.0658       0.0420       0.0089 47.9545
  197       0.0022       0.0641       0.0418       0.0085 49.8865
  198       0.0022       0.0643       0.0417       0.0085 49.9037
  199       0.0021       0.0641       0.0414       0.0084 49.9514
  200       0.0019       0.0632       0.0413       0.0082 49.7872
...Training Complete!

