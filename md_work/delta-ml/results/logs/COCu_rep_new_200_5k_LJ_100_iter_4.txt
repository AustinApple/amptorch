Fri Mar 13 10:53:04 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [0.972, 6.379], 'O': [1.09, 8.575], 'Cu': [2.168, 3.8386]}
Optimizer results: 
 fun: 1.4031175645961085 
 message: Max. number of function evaluations reached 
 nfev: 100 
 nit: 18 
 success: False
Fitted LJ parameters: {'C': array([1.        , 0.00189726]), 'O': array([1.        , 0.63255802]), 'Cu': array([1.38495238, 0.17549277])} 

a: 5.822498797522996
Optimization time: 2312.4332060813904 

Filename: COCu_rep_new_200_5k_LJ_100_iter_4
Dataset size: 600
Target scaling: [8.841315589518457, -6.0388791791387675]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 600
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.0404       0.5881     136.9541       0.9281 80.4796
    2       0.0342       0.5590     134.8442       0.8202 84.4338
    3       0.0442       0.6312     132.3530       1.0735 84.6885
    4       0.0323       0.4440     127.6717       0.5356 84.2894
    5       0.0262       0.5666     120.6331       0.8117 84.5940
    6       0.0611       0.6039     115.6846       1.0992 84.2989
    7       0.0493       0.6104     108.2440       1.0403 83.1695
    8       0.0680       0.6281     103.8266       1.2240 79.9263
    9       0.0639       0.5974      95.0475       1.1016 77.3005
   10       0.0546       0.5793      91.0212       0.9845 80.1449
   11       0.0526       0.4468      85.7426       0.6452 78.4474
   12       0.0334       0.5440      79.2323       0.7770 77.8862
   13       0.0595       0.4269      72.0510       0.6498 79.2475
   14       0.0385       0.4003      62.9823       0.4737 78.5481
   15       0.0342       0.3827      52.0158       0.4216 79.1569
   16       0.0392       0.3151      45.5252       0.3307 79.1049
   17       0.0309       0.3056      41.8505       0.2813 76.4913
   18       0.0408       0.3694      37.2397       0.4273 76.3220
   19       0.0376       0.3202      34.0013       0.3310 76.4545
   20       0.0367       0.2609      31.2296       0.2444 76.5032
   21       0.0341       0.2530      29.7341       0.2236 79.9812
   22       0.0319       0.2441      28.4345       0.2039 79.9896
   23       0.0267       0.2505      26.5321       0.1934 78.5991
   24       0.0385       0.3158      24.2252       0.3282 78.8323
   25       0.0377       0.3298      21.6053       0.3465 76.0348
   26       0.0330       0.3225      19.6530       0.3151 75.4527
   27       0.0360       0.3023      18.2179       0.2972 78.6985
   28       0.0366       0.3280      16.8470       0.3384 76.3608
   29       0.0362       0.3363      15.3192       0.3501 79.5910
   30       0.0340       0.3390      14.3621       0.3451 76.4651
   31       0.0403       0.3711      13.2058       0.4278 79.5175
   32       0.0438       0.3395      11.9002       0.3919 79.8290
   33       0.0400       0.3286      10.8471       0.3554 75.5813
   34       0.0380       0.3127      10.1767       0.3213 76.2308
   35       0.0358       0.3440       8.6746       0.3608 90.7977
   36       0.0320       0.3468       8.0599       0.3499 81.8657
   37       0.0328       0.3524       7.5684       0.3628 84.3299
   38       0.0343       0.3220       6.9763       0.3195 88.5635
   39       0.0359       0.3184       6.6071       0.3208 85.3367
   40       0.0351       0.3482       6.3282       0.3648 84.8171
   41       0.0368       0.3276       6.0629       0.3387 87.6169
   42       0.0347       0.3071       5.8233       0.2986 85.1929
   43       0.0332       0.3111       5.4181       0.2984 84.5513
   44       0.0321       0.3384       5.3029       0.3365 88.1903
   45       0.0312       0.3290       5.1721       0.3183 84.0725
   46       0.0273       0.3035       5.0633       0.2658 87.4362
   47       0.0260       0.3057       4.9236       0.2649 85.1707
   48       0.0257       0.3127       4.8156       0.2744 87.9369
   49       0.0252       0.2971       4.7592       0.2500 84.4272
   50       0.0252       0.2998       4.6694       0.2537 84.6428
   51       0.0242       0.3103       4.6097       0.2663 84.0139
   52       0.0242       0.3040       4.5631       0.2571 84.0356
   53       0.0253       0.2998       4.5160       0.2542 85.1873
   54       0.0244       0.2793       4.4039       0.2230 75.8007
   55       0.0259       0.2873       4.3257       0.2384 72.5829
   56       0.0264       0.3156       4.2216       0.2809 69.6022
   57       0.0262       0.3194       4.1014       0.2861 71.4230
   58       0.0278       0.3157       4.0378       0.2854 69.2633
   59       0.0283       0.3005       3.9878       0.2647 72.1435
   60       0.0260       0.2872       3.9173       0.2385 69.1907
   61       0.0249       0.2829       3.8449       0.2292 71.8151
   62       0.0252       0.2716       3.8095       0.2150 69.6741
   63       0.0248       0.2611       3.7498       0.2004 72.4065
   64       0.0246       0.2580       3.6995       0.1960 71.6198
   65       0.0243       0.2549       3.6796       0.1915 71.9311
   66       0.0263       0.2555       3.6590       0.1982 71.1565
   67       0.0265       0.2614       3.6084       0.2060 71.7647
   68       0.0264       0.2652       3.5921       0.2106 72.0627
   69       0.0265       0.2579       3.5757       0.2018 69.7025
   70       0.0276       0.2386       3.5543       0.1824 68.9476
   71       0.0277       0.2399       3.5157       0.1843 71.6363
   72       0.0276       0.2386       3.4718       0.1825 72.4536
   73       0.0276       0.2415       3.4366       0.1857 72.0738
   74       0.0287       0.2454       3.4058       0.1941 69.1909
   75       0.0284       0.2536       3.3658       0.2027 72.1722
   76       0.0277       0.2433       3.3503       0.1880 69.3780
   77       0.0280       0.2380       3.3206       0.1829 70.1264
   78       0.0269       0.2397       3.2788       0.1812 68.7881
   79       0.0263       0.2368       3.2614       0.1760 68.7373
   80       0.0270       0.2295       3.2392       0.1703 69.9479
   81       0.0265       0.2264       3.2082       0.1650 68.5025
   82       0.0248       0.2272       3.1827       0.1607 71.9139
   83       0.0261       0.2132       3.1596       0.1500 72.9680
   84       0.0243       0.2147       3.1108       0.1461 72.3039
   85       0.0241       0.2161       3.0900       0.1469 72.4005
   86       0.0234       0.2185       3.0721       0.1476 71.9171
   87       0.0230       0.2135       3.0482       0.1410 71.7155
   88       0.0223       0.2123       3.0244       0.1379 69.0529
   89       0.0217       0.2105       2.9909       0.1345 82.3727
   90       0.0214       0.2104       2.9693       0.1337 76.0277
   91       0.0211       0.2085       2.9261       0.1310 77.5428
   92       0.0211       0.2071       2.9030       0.1296 74.8355
   93       0.0209       0.2096       2.8850       0.1315 75.7075
   94       0.0215       0.2192       2.8507       0.1430 74.0765
   95       0.0210       0.2216       2.8193       0.1443 77.3887
   96       0.0209       0.2179       2.7971       0.1401 75.5433
   97       0.0206       0.2188       2.7647       0.1404 75.6414
   98       0.0205       0.2176       2.7456       0.1389 75.8651
   99       0.0216       0.2169       2.7189       0.1408 72.3323
  100       0.0211       0.2157       2.6694       0.1383 67.8360
  101       0.0213       0.2190       2.6422       0.1423 78.0044
  102       0.0211       0.2177       2.6172       0.1403 76.1586
  103       0.0220       0.2142       2.6006       0.1390 78.8299
  104       0.0223       0.2157       2.5729       0.1414 75.0329
  105       0.0224       0.2246       2.5460       0.1513 75.2360
  106       0.0233       0.2266       2.5035       0.1557 79.6304
  107       0.0234       0.2228       2.4894       0.1519 76.0090
  108       0.0241       0.2210       2.4714       0.1521 75.9209
  109       0.0233       0.2272       2.4495       0.1566 76.1590
  110       0.0222       0.2284       2.4252       0.1549 75.5159
  111       0.0209       0.2330       2.4035       0.1564 75.6290
  112       0.0216       0.2319       2.3824       0.1570 76.1039
  113       0.0209       0.2331       2.3566       0.1567 78.8359
  114       0.0212       0.2287       2.3258       0.1525 76.2611
  115       0.0212       0.2283       2.3021       0.1520 75.9168
  116       0.0214       0.2266       2.2884       0.1508 78.9627
  117       0.0220       0.2271       2.2767       0.1529 75.8255
  118       0.0222       0.2273       2.2651       0.1536 75.9287
  119       0.0220       0.2243       2.2523       0.1498 77.9843
  120       0.0217       0.2219       2.2332       0.1465 75.6344
  121       0.0215       0.2179       2.2020       0.1418 75.3272
  122       0.0208       0.2111       2.1901       0.1330 78.5764
  123       0.0210       0.2084       2.1733       0.1306 78.5913
  124       0.0214       0.2055       2.1635       0.1289 75.4467
  125       0.0222       0.2046       2.1480       0.1300 78.9426
  126       0.0226       0.2077       2.1364       0.1342 72.9676
  127       0.0221       0.2098       2.1223       0.1350 78.1320
  128       0.0235       0.2064       2.1091       0.1354 76.1765
  129       0.0238       0.2050       2.0939       0.1348 73.1734
  130       0.0228       0.2048       2.0814       0.1319 79.4048
  131       0.0228       0.2055       2.0632       0.1325 75.4451
  132       0.0223       0.2081       2.0557       0.1339 79.0503
  133       0.0225       0.2067       2.0374       0.1328 76.0180
  134       0.0235       0.2028       2.0324       0.1319 78.9846
  135       0.0236       0.2025       2.0234       0.1318 75.5879
  136       0.0226       0.2023       2.0165       0.1290 76.3477
  137       0.0225       0.2040       2.0121       0.1303 75.4687
  138       0.0226       0.2026       2.0061       0.1291 71.0615
  139       0.0222       0.2045       2.0009       0.1301 70.5011
  140       0.0226       0.1999       1.9919       0.1265 72.2207
  141       0.0226       0.2010       1.9844       0.1275 69.7836
  142       0.0228       0.2022       1.9769       0.1294 72.8859
  143       0.0227       0.2019       1.9656       0.1287 69.8608
  144       0.0226       0.1986       1.9569       0.1252 69.0427
  145       0.0231       0.1954       1.9431       0.1235 68.5573
  146       0.0237       0.1904       1.9355       0.1208 71.8007
  147       0.0240       0.1908       1.9298       0.1219 71.8584
  148       0.0239       0.1912       1.9250       0.1220 69.0394
  149       0.0236       0.1936       1.9202       0.1235 71.8823
  150       0.0233       0.1963       1.9167       0.1251 69.3857
  151       0.0234       0.1953       1.9123       0.1243 71.4928
  152       0.0233       0.1967       1.9075       0.1254 69.4231
  153       0.0234       0.1985       1.9007       0.1274 69.1627
  154       0.0236       0.1994       1.8944       0.1288 71.1354
  155       0.0232       0.1990       1.8895       0.1275 68.7887
  156       0.0233       0.1954       1.8826       0.1243 69.2996
  157       0.0228       0.1975       1.8797       0.1247 68.8921
  158       0.0228       0.1974       1.8755       0.1248 68.9944
  159       0.0228       0.2001       1.8725       0.1273 71.7718
  160       0.0225       0.1986       1.8638       0.1252 69.3956
  161       0.0226       0.2004       1.8552       0.1272 69.0819
  162       0.0223       0.2009       1.8518       0.1269 69.2778
  163       0.0221       0.1992       1.8491       0.1245 71.8009
  164       0.0224       0.1986       1.8468       0.1248 68.9500
  165       0.0227       0.1970       1.8442       0.1240 69.5821
  166       0.0227       0.1966       1.8397       0.1236 65.2285
  167       0.0228       0.1921       1.8361       0.1198 79.3617
  168       0.0229       0.1904       1.8341       0.1184 66.3431
  169       0.0230       0.1914       1.8286       0.1197 68.8562
  170       0.0230       0.1931       1.8259       0.1212 72.2393
  171       0.0229       0.1917       1.8234       0.1196 69.0864
  172       0.0235       0.1901       1.8203       0.1198 71.6678
  173       0.0236       0.1886       1.8169       0.1187 71.9898
  174       0.0237       0.1869       1.8132       0.1174 71.6069
  175       0.0232       0.1887       1.8085       0.1178 69.6288
  176       0.0231       0.1919       1.8054       0.1203 71.7924
  177       0.0231       0.1927       1.8018       0.1212 69.4116
  178       0.0227       0.1985       1.7953       0.1254 72.9126
  179       0.0226       0.1993       1.7919       0.1260 69.0506
  180       0.0227       0.1962       1.7893       0.1234 72.3916
  181       0.0228       0.1942       1.7868       0.1219 68.9304
  182       0.0231       0.1924       1.7844       0.1207 71.1576
  183       0.0233       0.1954       1.7809       0.1243 72.0250
  184       0.0238       0.1925       1.7764       0.1229 71.1980
  185       0.0237       0.1954       1.7738       0.1253 71.7708
  186       0.0235       0.1970       1.7699       0.1262 68.7601
  187       0.0246       0.1905       1.7676       0.1233 70.6847
  188       0.0247       0.1883       1.7596       0.1217 68.7943
  189       0.0246       0.1877       1.7571       0.1209 71.8410
  190       0.0245       0.1882       1.7538       0.1210 71.5099
  191       0.0249       0.1860       1.7524       0.1204 68.9532
  192       0.0253       0.1867       1.7480       0.1219 71.9394
  193       0.0252       0.1880       1.7452       0.1228 69.6437
  194       0.0254       0.1881       1.7441       0.1237 69.7034
  195       0.0254       0.1901       1.7421       0.1253 69.1932
  196       0.0253       0.1895       1.7397       0.1247 71.8246
  197       0.0248       0.1897       1.7373       0.1234 69.7908
  198       0.0242       0.1898       1.7349       0.1217 69.6673
  199       0.0239       0.1879       1.7280       0.1190 71.9629
  200       0.0234       0.1897       1.7218       0.1192 71.9209
...Training Complete!

