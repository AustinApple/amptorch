Tue Mar 10 18:55:02 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [1.0, 6.3535, 1.5], 'O': [1.0808, 8.5357, 1.6], 'Cu': [2.1717, 3.7575, 1.5]}
Optimizer results: 
 fun: 0.047365911827893785 
 message: Max. number of function evaluations reached 
 nfev: 100 
 nit: 22 
 success: False
Fitted LJ parameters: {'C': array([1.        , 5.27731437, 1.        ]), 'O': array([1.11080477, 7.68460864, 1.        ]), 'Cu': array([ 1.        , 10.        ,  1.14807879])} 

Optimization time: 472.43805146217346 

Filename: COCu_morse_newfit_f100_LJ
Dataset size: 100
Target scaling: [8.841315589518457, 2097.343289755859]
Symmetry function parameters:
     G2_etas: [0.05       0.23207944 1.07721735 5.        ]
     G2_rs_s: [0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 60
   # of Hidden Layers - 2
   Nodes/Layer - 20
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 100
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.0861       0.0943       0.9740       0.0778  9.7140
    2       0.0860       0.0841       0.8277       0.0768  9.2705
    3       0.0859       0.0942       0.8194       0.0773  9.2576
    4       0.0854       0.1034       0.8040       0.0772  9.6221
    5       0.0860       0.0622       0.7894       0.0754  9.4882
    6       0.0863       0.0594       0.7692       0.0759  8.8542
    7       0.0860       0.0755       0.7646       0.0762  9.8195
    8       0.0863       0.0596       0.7593       0.0759  9.2041
    9       0.0860       0.0582       0.7565       0.0753  8.3171
   10       0.0860       0.0614       0.7542       0.0754  7.5149
   11       0.0858       0.0857       0.7531       0.0765  5.8119
   12       0.0855       0.0976       0.7519       0.0770  5.7979
   13       0.0857       0.0966       0.7505       0.0771  5.7210
   14       0.0857       0.0861       0.7500       0.0764  5.9439
   15       0.0855       0.0885       0.7496       0.0763  5.8918
   16       0.0857       0.0770       0.7486       0.0758  5.7402
   17       0.0858       0.0801       0.7479       0.0761  5.8684
   18       0.0857       0.0933       0.7470       0.0770  6.0503
   19       0.0857       0.0890       0.7466       0.0767  5.9393
   20       0.0860       0.0741       0.7460       0.0761  5.8193
   21       0.0861       0.0774       0.7453       0.0766  5.8183
   22       0.0864       0.0941       0.7447       0.0781  6.0677
   23       0.0863       0.1143       0.7441       0.0798  5.7965
   24       0.0865       0.1329       0.7436       0.0819  6.1567
   25       0.0863       0.1162       0.7427       0.0798  6.0235
   26       0.0867       0.1741       0.7417       0.0874  5.7051
   27       0.0871       0.2294       0.7405       0.0968  5.8872
   28       0.0865       0.1926       0.7384       0.0896  5.9566
   29       0.0851       0.1053       0.7365       0.0769  5.7249
   30       0.0845       0.1969       0.7317       0.0868  5.7990
   31       0.0850       0.2188       0.7255       0.0914  5.0735
   32       0.0846       0.2530       0.7201       0.0972  5.8783
   33       0.0845       0.2978       0.7149       0.1069  5.7363
   34       0.0840       0.3336       0.7119       0.1151  5.6943
   35       0.0841       0.3527       0.7099       0.1205  5.9175
   36       0.0841       0.3541       0.7083       0.1209  5.6222
   37       0.0838       0.3919       0.7072       0.1317  6.0747
   38       0.0836       0.4266       0.7052       0.1427  5.7362
   39       0.0835       0.4454       0.7043       0.1491  5.8693
   40       0.0836       0.4788       0.7033       0.1616  5.8588
   41       0.0835       0.4964       0.7028       0.1683  5.8596
   42       0.0834       0.5145       0.7023       0.1754  5.7437
   43       0.0834       0.5222       0.7017       0.1787  6.0261
   44       0.0832       0.5406       0.7012       0.1861  5.8700
   45       0.0830       0.5420       0.7008       0.1864  5.7615
   46       0.0828       0.5402       0.7003       0.1853  5.6917
   47       0.0826       0.5632       0.6998       0.1952  5.9998
   48       0.0826       0.5622       0.6995       0.1947  5.7327
   49       0.0823       0.5790       0.6991       0.2018  5.8049
   50       0.0821       0.5897       0.6986       0.2064  5.8828
   51       0.0820       0.5712       0.6980       0.1978  5.1577
   52       0.0820       0.5762       0.6976       0.2000  5.8888
   53       0.0817       0.6048       0.6974       0.2131  5.6483
   54       0.0815       0.6432       0.6969       0.2319  5.3601
   55       0.0813       0.6655       0.6964       0.2432  9.8247
   56       0.0812       0.6733       0.6961       0.2472  9.7135
   57       0.0810       0.6922       0.6959       0.2572  7.2020
   58       0.0809       0.7038       0.6955       0.2636  7.3272
   59       0.0808       0.7177       0.6954       0.2713  7.0541
   60       0.0808       0.7172       0.6951       0.2711  6.9826
   61       0.0805       0.7314       0.6949       0.2788  6.2880
   62       0.0803       0.7623       0.6947       0.2969  6.6109
   63       0.0801       0.7862       0.6945       0.3114  6.5524
   64       0.0799       0.8149       0.6943       0.3294  6.6368
   65       0.0797       0.8358       0.6942       0.3430  6.4654
   66       0.0798       0.8400       0.6940       0.3458  6.3000
   67       0.0797       0.8415       0.6939       0.3468  6.1904
   68       0.0797       0.8536       0.6938       0.3550  6.5828
   69       0.0797       0.8427       0.6937       0.3477  6.2139
   70       0.0796       0.8549       0.6936       0.3558  6.5619
   71       0.0796       0.8411       0.6935       0.3463  6.4326
   72       0.0796       0.8341       0.6934       0.3417  6.4931
   73       0.0796       0.8209       0.6933       0.3329  6.7344
   74       0.0797       0.8072       0.6932       0.3242  6.3201
   75       0.0797       0.8035       0.6930       0.3217  6.3766
   76       0.0795       0.7964       0.6929       0.3170  6.3009
   77       0.0795       0.7961       0.6928       0.3167  6.5182
   78       0.0794       0.7837       0.6927       0.3088  6.3524
   79       0.0794       0.7603       0.6927       0.2943  6.6222
   80       0.0793       0.7501       0.6925       0.2880  6.4981
   81       0.0793       0.7418       0.6924       0.2830  6.3234
   82       0.0791       0.7363       0.6923       0.2795  6.3881
   83       0.0791       0.7186       0.6922       0.2692  6.5442
   84       0.0793       0.6895       0.6922       0.2530  6.2106
   85       0.0793       0.6681       0.6921       0.2415  6.5477
   86       0.0794       0.6515       0.6920       0.2327  6.3295
   87       0.0794       0.6339       0.6920       0.2237  6.4422
   88       0.0794       0.6277       0.6919       0.2206  6.5831
   89       0.0793       0.6168       0.6918       0.2151  6.4301
   90       0.0793       0.5990       0.6918       0.2064  6.2006
   91       0.0793       0.5951       0.6917       0.2045  6.5657
   92       0.0793       0.5686       0.6916       0.1923  6.7165
   93       0.0794       0.5500       0.6915       0.1840  6.7280
   94       0.0794       0.5413       0.6914       0.1802  6.6441
   95       0.0793       0.5405       0.6913       0.1798  6.1730
   96       0.0794       0.5334       0.6913       0.1768  6.5537
   97       0.0795       0.5199       0.6912       0.1713  6.2662
   98       0.0795       0.5143       0.6912       0.1690  6.2127
   99       0.0795       0.5105       0.6911       0.1675  6.4076
  100       0.0796       0.4922       0.6911       0.1603  6.5009
  101       0.0796       0.4845       0.6910       0.1573  6.6475
  102       0.0797       0.4765       0.6910       0.1543  6.5470
  103       0.0797       0.4770       0.6909       0.1545  6.6988
  104       0.0797       0.4659       0.6909       0.1503  6.5738
  105       0.0797       0.4665       0.6908       0.1505  7.0699
  106       0.0797       0.4634       0.6908       0.1494  6.4949
  107       0.0797       0.4620       0.6907       0.1488  6.5067
  108       0.0797       0.4491       0.6906       0.1442  6.5555
  109       0.0798       0.4342       0.6906       0.1391  6.2857
  110       0.0798       0.4282       0.6905       0.1371  6.3081
  111       0.0799       0.4151       0.6905       0.1328  6.5301
  112       0.0800       0.3984       0.6904       0.1275  6.5075
  113       0.0800       0.3923       0.6904       0.1256  6.2708
  114       0.0800       0.3933       0.6904       0.1259  6.4836
  115       0.0801       0.3858       0.6903       0.1237  6.3067
  116       0.0801       0.3741       0.6903       0.1201  6.2949
  117       0.0801       0.3659       0.6902       0.1177  6.7020
  118       0.0801       0.3670       0.6902       0.1181  6.4225
  119       0.0801       0.3645       0.6902       0.1173  6.5802
  120       0.0801       0.3641       0.6901       0.1172  6.4108
  121       0.0801       0.3610       0.6900       0.1163  6.5177
  122       0.0802       0.3562       0.6900       0.1150  6.4505
  123       0.0802       0.3477       0.6899       0.1127  6.5447
  124       0.0803       0.3344       0.6899       0.1093  6.2278
  125       0.0804       0.3222       0.6898       0.1062  6.4774
  126       0.0806       0.3098       0.6898       0.1033  6.6158
  127       0.0806       0.3050       0.6897       0.1022  6.5505
  128       0.0806       0.3040       0.6897       0.1020  6.2615
  129       0.0806       0.3042       0.6896       0.1020  6.3179
  130       0.0807       0.3012       0.6896       0.1014  6.1631
  131       0.0807       0.3001       0.6895       0.1011  6.6276
  132       0.0807       0.3016       0.6895       0.1014  6.4372
  133       0.0806       0.3049       0.6894       0.1022  6.1541
  134       0.0806       0.3038       0.6894       0.1019  6.2817
  135       0.0806       0.3038       0.6893       0.1020  6.5362
  136       0.0807       0.3008       0.6893       0.1013  6.4280
  137       0.0808       0.2976       0.6892       0.1006  6.8882
  138       0.0808       0.2943       0.6892       0.0999  6.6038
  139       0.0809       0.2902       0.6892       0.0991  6.4852
  140       0.0809       0.2862       0.6891       0.0982  6.3785
  141       0.0809       0.2877       0.6891       0.0986  6.6655
  142       0.0809       0.2921       0.6890       0.0995  6.4678
  143       0.0808       0.2963       0.6890       0.1004  6.7186
  144       0.0808       0.2946       0.6890       0.1001  6.6062
  145       0.0808       0.3015       0.6889       0.1016  6.7174
  146       0.0807       0.3043       0.6889       0.1022  6.3441
  147       0.0807       0.3098       0.6888       0.1035  6.5839
  148       0.0807       0.3095       0.6888       0.1035  6.0944
  149       0.0807       0.3092       0.6887       0.1034  6.3094
  150       0.0807       0.3137       0.6887       0.1045  6.4888
  151       0.0807       0.3172       0.6886       0.1054  6.6578
  152       0.0807       0.3191       0.6886       0.1058  6.5332
  153       0.0807       0.3208       0.6885       0.1062  6.2937
  154       0.0807       0.3206       0.6885       0.1062  6.4933
  155       0.0807       0.3212       0.6884       0.1064  6.5379
  156       0.0807       0.3235       0.6884       0.1070  6.4184
  157       0.0807       0.3206       0.6883       0.1063  6.4510
  158       0.0807       0.3280       0.6883       0.1082  6.5892
  159       0.0808       0.3271       0.6882       0.1080  6.5133
  160       0.0808       0.3263       0.6882       0.1079  6.2313
  161       0.0809       0.3237       0.6881       0.1074  6.7564
  162       0.0809       0.3316       0.6881       0.1094  6.7049
  163       0.0809       0.3332       0.6881       0.1098  6.1605
  164       0.0809       0.3326       0.6880       0.1097  6.8159
  165       0.0810       0.3354       0.6880       0.1106  6.3031
  166       0.0810       0.3389       0.6879       0.1115  6.3787
  167       0.0809       0.3407       0.6879       0.1119  6.2634
  168       0.0809       0.3432       0.6879       0.1126  6.5218
  169       0.0809       0.3420       0.6878       0.1123  6.3037
  170       0.0809       0.3419       0.6878       0.1123  6.3886
  171       0.0810       0.3366       0.6878       0.1109  6.3338
  172       0.0810       0.3363       0.6877       0.1108  6.6066
  173       0.0810       0.3361       0.6877       0.1108  7.6389
  174       0.0810       0.3370       0.6877       0.1111  6.4734
  175       0.0811       0.3336       0.6877       0.1103  6.3587
  176       0.0812       0.3326       0.6876       0.1101  6.4266
  177       0.0811       0.3346       0.6876       0.1106  6.4235
  178       0.0812       0.3345       0.6876       0.1106  6.3655
  179       0.0811       0.3384       0.6876       0.1116  6.4860
  180       0.0811       0.3402       0.6876       0.1120  6.8257
  181       0.0811       0.3381       0.6875       0.1115  6.2032
  182       0.0811       0.3354       0.6875       0.1107  6.3242
  183       0.0811       0.3354       0.6875       0.1107  3.7250
  184       0.0811       0.3354       0.6875       0.1107  1.9675
  185       0.0811       0.3354       0.6875       0.1107  1.9245
  186       0.0811       0.3354       0.6875       0.1107  2.0054
  187       0.0811       0.3354       0.6875       0.1107  1.9306
  188       0.0811       0.3354       0.6875       0.1107  2.0096
  189       0.0811       0.3354       0.6875       0.1107  1.9921
  190       0.0811       0.3354       0.6875       0.1107  2.0573
  191       0.0811       0.3354       0.6875       0.1107  2.0157
  192       0.0811       0.3354       0.6875       0.1107  2.1141
  193       0.0811       0.3354       0.6875       0.1107  2.0646
  194       0.0811       0.3354       0.6875       0.1107  1.9380
  195       0.0811       0.3354       0.6875       0.1107  1.9854
  196       0.0811       0.3354       0.6875       0.1107  1.9606
  197       0.0811       0.3354       0.6875       0.1107  2.0211
  198       0.0811       0.3354       0.6875       0.1107  1.9493
  199       0.0811       0.3354       0.6875       0.1107  1.9171
  200       0.0811       0.3354       0.6875       0.1107  2.1144
...Training Complete!

