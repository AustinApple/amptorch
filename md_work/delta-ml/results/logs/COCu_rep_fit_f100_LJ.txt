Tue Mar 10 13:39:35 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [1.0, 6.3535], 'O': [1.0808, 8.5357], 'Cu': [2.1717, 3.7575]}
Optimizer results: 
 fun: 0.050450749930789796 
 message: Maximum number of function evaluations has been exceeded. 
 nfev: 1400 
 nit: 910 
 success: False
Fitted LJ parameters: {'C': array([-0.07958097, 10.4855147 ]), 'O': array([2.01497818e+00, 1.99652325e-04]), 'Cu': array([2.19368377, 3.49970224])} 

a: 36.7880031596305
Optimization time: 493.06511068344116 

Filename: COCu_rep_fit_f100_LJ
Dataset size: 100
Target scaling: [8.841315589518457, 0.2090448430698702]
Symmetry function parameters:
     G2_etas: [0.05       0.23207944 1.07721735 5.        ]
     G2_rs_s: [0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 60
   # of Hidden Layers - 2
   Nodes/Layer - 20
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 100
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.0012       0.1386       0.5225       0.0077  5.8062
    2       0.0017       0.1048       0.0431       0.0044  5.8888
    3       0.0025       0.0717       0.0219       0.0021  5.8293
    4       0.0018       0.0701       0.0155       0.0020  5.6437
    5       0.0009       0.0468       0.0121       0.0009  5.7803
    6       0.0012       0.0364       0.0067       0.0005  5.6045
    7       0.0004       0.0365       0.0041       0.0005  5.7527
    8       0.0006       0.0374       0.0029       0.0006  5.8933
    9       0.0003       0.0302       0.0022       0.0004  5.6759
   10       0.0004       0.0303       0.0019       0.0004  5.7877
   11       0.0004       0.0281       0.0018       0.0003  5.6837
   12       0.0005       0.0299       0.0017       0.0004  6.0083
   13       0.0005       0.0287       0.0015       0.0003  5.9049
   14       0.0006       0.0295       0.0015       0.0004  5.8307
   15       0.0004       0.0300       0.0014       0.0004  6.0294
   16       0.0005       0.0299       0.0013       0.0004  5.8352
   17       0.0006       0.0265       0.0012       0.0003  5.7090
   18       0.0004       0.0274       0.0012       0.0003  5.6826
   19       0.0006       0.0255       0.0012       0.0003  5.6813
   20       0.0006       0.0263       0.0011       0.0003  5.7207
   21       0.0006       0.0238       0.0011       0.0002  5.6448
   22       0.0005       0.0239       0.0010       0.0002  5.7892
   23       0.0004       0.0241       0.0010       0.0002  5.6414
   24       0.0004       0.0232       0.0010       0.0002  5.7052
   25       0.0005       0.0225       0.0009       0.0002  5.6552
   26       0.0003       0.0225       0.0009       0.0002  5.8587
   27       0.0004       0.0225       0.0009       0.0002  6.1229
   28       0.0004       0.0227       0.0009       0.0002  5.8755
   29       0.0003       0.0218       0.0008       0.0002  6.0280
   30       0.0004       0.0193       0.0008       0.0002  5.5594
   31       0.0004       0.0187       0.0008       0.0001  5.9017
   32       0.0004       0.0182       0.0008       0.0001  5.9461
   33       0.0005       0.0165       0.0008       0.0001  5.7298
   34       0.0005       0.0159       0.0008       0.0001  5.6240
   35       0.0004       0.0156       0.0007       0.0001  5.9677
   36       0.0004       0.0156       0.0007       0.0001  5.9578
   37       0.0003       0.0143       0.0007       0.0001  5.7795
   38       0.0003       0.0147       0.0007       0.0001  5.6099
   39       0.0004       0.0148       0.0007       0.0001  5.6232
   40       0.0004       0.0147       0.0007       0.0001  5.8011
   41       0.0004       0.0154       0.0007       0.0001  5.6704
   42       0.0003       0.0152       0.0007       0.0001  5.6707
   43       0.0002       0.0148       0.0007       0.0001  5.6180
   44       0.0003       0.0162       0.0007       0.0001  5.4957
   45       0.0004       0.0153       0.0007       0.0001  5.6657
   46       0.0003       0.0143       0.0007       0.0001  4.9073
   47       0.0004       0.0138       0.0006       0.0001  5.6068
   48       0.0004       0.0134       0.0006       0.0001  5.8471
   49       0.0004       0.0132       0.0006       0.0001  5.8315
   50       0.0004       0.0134       0.0006       0.0001  5.7073
   51       0.0004       0.0134       0.0006       0.0001  5.2814
   52       0.0003       0.0136       0.0006       0.0001  5.3133
   53       0.0003       0.0134       0.0006       0.0001  5.3347
   54       0.0004       0.0133       0.0006       0.0001  5.2108
   55       0.0003       0.0124       0.0006       0.0001  5.4116
   56       0.0004       0.0121       0.0006       0.0001  5.4437
   57       0.0004       0.0122       0.0006       0.0001  5.4241
   58       0.0004       0.0118       0.0006       0.0001  5.4936
   59       0.0004       0.0113       0.0005       0.0001  5.4185
   60       0.0004       0.0114       0.0005       0.0001  5.3042
   61       0.0004       0.0113       0.0005       0.0001  5.5740
   62       0.0003       0.0113       0.0005       0.0001  5.0199
   63       0.0003       0.0128       0.0005       0.0001  5.1734
   64       0.0004       0.0136       0.0005       0.0001  5.2456
   65       0.0004       0.0137       0.0005       0.0001  5.2160
   66       0.0004       0.0151       0.0005       0.0001  5.1399
   67       0.0004       0.0166       0.0005       0.0001  5.0182
   68       0.0004       0.0156       0.0005       0.0001  5.1720
   69       0.0004       0.0209       0.0005       0.0002  5.2561
   70       0.0004       0.0226       0.0005       0.0002  5.2157
   71       0.0004       0.0223       0.0005       0.0002  4.6694
   72       0.0004       0.0217       0.0005       0.0002  5.0632
   73       0.0004       0.0192       0.0005       0.0001  5.3310
   74       0.0004       0.0184       0.0005       0.0001  5.3020
   75       0.0004       0.0186       0.0005       0.0001  5.5628
   76       0.0004       0.0173       0.0005       0.0001  5.2868
   77       0.0003       0.0175       0.0004       0.0001  5.2422
   78       0.0004       0.0183       0.0004       0.0001  5.3059
   79       0.0004       0.0171       0.0004       0.0001  5.1530
   80       0.0004       0.0161       0.0004       0.0001  5.3270
   81       0.0004       0.0160       0.0004       0.0001  5.3519
   82       0.0004       0.0154       0.0004       0.0001  5.4212
   83       0.0003       0.0161       0.0004       0.0001  5.4473
   84       0.0004       0.0170       0.0004       0.0001  5.2167
   85       0.0004       0.0158       0.0004       0.0001  5.0535
   86       0.0004       0.0170       0.0004       0.0001  4.9738
   87       0.0003       0.0162       0.0004       0.0001  5.0294
   88       0.0003       0.0151       0.0004       0.0001  5.1574
   89       0.0004       0.0155       0.0004       0.0001  5.0656
   90       0.0003       0.0146       0.0004       0.0001  5.0092
   91       0.0003       0.0138       0.0004       0.0001  5.2114
   92       0.0003       0.0133       0.0004       0.0001  5.1354
   93       0.0003       0.0140       0.0004       0.0001  4.9720
   94       0.0003       0.0136       0.0004       0.0001  5.0349
   95       0.0003       0.0129       0.0004       0.0001  5.3204
   96       0.0002       0.0127       0.0004       0.0001  4.4954
   97       0.0002       0.0124       0.0004       0.0001  5.0392
   98       0.0002       0.0124       0.0004       0.0001  5.2369
   99       0.0002       0.0127       0.0004       0.0001  5.1736
  100       0.0002       0.0129       0.0004       0.0001  5.2380
  101       0.0002       0.0145       0.0004       0.0001  5.1739
  102       0.0002       0.0144       0.0003       0.0001  5.1393
  103       0.0002       0.0146       0.0003       0.0001  5.2442
  104       0.0002       0.0144       0.0003       0.0001  5.3076
  105       0.0002       0.0136       0.0003       0.0001  5.0723
  106       0.0002       0.0135       0.0003       0.0001  5.2088
  107       0.0002       0.0133       0.0003       0.0001  5.1827
  108       0.0002       0.0130       0.0003       0.0001  5.0982
  109       0.0002       0.0127       0.0003       0.0001  4.9787
  110       0.0002       0.0127       0.0003       0.0001  5.2862
  111       0.0002       0.0126       0.0003       0.0001  5.0018
  112       0.0002       0.0125       0.0003       0.0001  5.2703
  113       0.0002       0.0130       0.0003       0.0001  5.2597
  114       0.0002       0.0130       0.0003       0.0001  5.2950
  115       0.0002       0.0128       0.0003       0.0001  5.9258
  116       0.0002       0.0127       0.0003       0.0001  8.0991
  117       0.0002       0.0120       0.0003       0.0001  8.3741
  118       0.0002       0.0128       0.0003       0.0001  8.8102
  119       0.0001       0.0130       0.0003       0.0001  8.4879
  120       0.0001       0.0138       0.0003       0.0001  5.7891
  121       0.0001       0.0131       0.0003       0.0001  5.0121
  122       0.0001       0.0128       0.0003       0.0001  5.1658
  123       0.0001       0.0125       0.0003       0.0001  4.9816
  124       0.0002       0.0127       0.0003       0.0001  4.9816
  125       0.0001       0.0126       0.0003       0.0001  5.1364
  126       0.0002       0.0132       0.0003       0.0001  5.2103
  127       0.0001       0.0132       0.0003       0.0001  5.0436
  128       0.0001       0.0129       0.0003       0.0001  5.0961
  129       0.0002       0.0134       0.0003       0.0001  5.2224
  130       0.0002       0.0136       0.0003       0.0001  5.1130
  131       0.0001       0.0134       0.0003       0.0001  5.1100
  132       0.0002       0.0129       0.0003       0.0001  5.1144
  133       0.0002       0.0134       0.0003       0.0001  5.0013
  134       0.0001       0.0134       0.0003       0.0001  5.0404
  135       0.0001       0.0137       0.0003       0.0001  4.8444
  136       0.0001       0.0136       0.0003       0.0001  4.8673
  137       0.0002       0.0131       0.0003       0.0001  5.0533
  138       0.0002       0.0131       0.0003       0.0001  4.8323
  139       0.0002       0.0131       0.0003       0.0001  4.7890
  140       0.0002       0.0130       0.0002       0.0001  4.9981
  141       0.0002       0.0130       0.0002       0.0001  4.9816
  142       0.0002       0.0130       0.0002       0.0001  4.8030
  143       0.0002       0.0125       0.0002       0.0001  4.9362
  144       0.0002       0.0123       0.0002       0.0001  4.8047
  145       0.0002       0.0126       0.0002       0.0001  4.8425
  146       0.0002       0.0121       0.0002       0.0001  4.9620
  147       0.0002       0.0128       0.0002       0.0001  4.8258
  148       0.0002       0.0137       0.0002       0.0001  4.9153
  149       0.0002       0.0139       0.0002       0.0001  4.9315
  150       0.0002       0.0138       0.0002       0.0001  5.0096
  151       0.0002       0.0138       0.0002       0.0001  4.7554
  152       0.0002       0.0136       0.0002       0.0001  4.7767
  153       0.0002       0.0137       0.0002       0.0001  4.9382
  154       0.0002       0.0136       0.0002       0.0001  4.7808
  155       0.0002       0.0139       0.0002       0.0001  4.9591
  156       0.0002       0.0137       0.0002       0.0001  4.9956
  157       0.0002       0.0132       0.0002       0.0001  4.7815
  158       0.0002       0.0134       0.0002       0.0001  4.7877
  159       0.0002       0.0131       0.0002       0.0001  4.8093
  160       0.0002       0.0128       0.0002       0.0001  4.9493
  161       0.0002       0.0131       0.0002       0.0001  4.7742
  162       0.0002       0.0129       0.0002       0.0001  4.7662
  163       0.0002       0.0126       0.0002       0.0001  4.7713
  164       0.0002       0.0126       0.0002       0.0001  4.8124
  165       0.0002       0.0128       0.0002       0.0001  4.7882
  166       0.0002       0.0131       0.0002       0.0001  4.7962
  167       0.0002       0.0129       0.0002       0.0001  4.9696
  168       0.0002       0.0130       0.0002       0.0001  4.7406
  169       0.0002       0.0132       0.0002       0.0001  4.7610
  170       0.0002       0.0134       0.0002       0.0001  4.9197
  171       0.0001       0.0136       0.0002       0.0001  4.7321
  172       0.0001       0.0139       0.0002       0.0001  4.7476
  173       0.0001       0.0143       0.0002       0.0001  4.8143
  174       0.0001       0.0147       0.0002       0.0001  4.9159
  175       0.0001       0.0148       0.0002       0.0001  4.7192
  176       0.0001       0.0155       0.0002       0.0001  4.9207
  177       0.0001       0.0151       0.0002       0.0001  4.9279
  178       0.0001       0.0148       0.0002       0.0001  4.5495
  179       0.0001       0.0152       0.0002       0.0001  4.7097
  180       0.0001       0.0167       0.0002       0.0001  4.5846
  181       0.0001       0.0172       0.0002       0.0001  4.5550
  182       0.0001       0.0178       0.0002       0.0001  4.6703
  183       0.0001       0.0176       0.0002       0.0001  4.6008
  184       0.0001       0.0187       0.0002       0.0001  4.7562
  185       0.0001       0.0189       0.0002       0.0001  4.5931
  186       0.0001       0.0196       0.0002       0.0002  4.5932
  187       0.0001       0.0193       0.0002       0.0001  4.7607
  188       0.0001       0.0201       0.0002       0.0002  4.6066
  189       0.0001       0.0201       0.0002       0.0002  4.5908
  190       0.0001       0.0204       0.0002       0.0002  4.6460
  191       0.0001       0.0207       0.0002       0.0002  4.5971
  192       0.0001       0.0219       0.0002       0.0002  4.5889
  193       0.0001       0.0221       0.0002       0.0002  4.7509
  194       0.0001       0.0216       0.0002       0.0002  4.5715
  195       0.0001       0.0217       0.0002       0.0002  4.7487
  196       0.0001       0.0222       0.0002       0.0002  4.6698
  197       0.0001       0.0221       0.0002       0.0002  4.5771
  198       0.0001       0.0226       0.0002       0.0002  4.5861
  199       0.0001       0.0230       0.0002       0.0002  4.7527
  200       0.0001       0.0232       0.0002       0.0002  4.5714
...Training Complete!

