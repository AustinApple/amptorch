Fri Mar 13 10:54:19 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [0.972, 6.379, 0.477], 'O': [1.09, 8.575, 0.603], 'Cu': [2.168, 3.8386, 1.696]}
Optimizer results: 
 fun: 97.98528243094196 
 message: Max. number of function evaluations reached 
 nfev: 100 
 nit: 24 
 success: False
Fitted LJ parameters: {'C': array([1.47761286, 6.24980613, 1.        ]), 'O': array([1.42224742, 8.46290245, 1.        ]), 'Cu': array([2.13837333, 3.65524967, 1.32933199])} 

Optimization time: 3191.3245244026184 

Filename: COCu_morse_new_200_5k_LJ_100_iter_4
Dataset size: 600
Target scaling: [8.841315589518457, -827.2946213958955]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 600
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.7323       6.9052    5760.5010     146.6167 74.7215
    2       0.2472       4.3817    1227.5989      49.7456 71.6350
    3       0.3111       4.3256     861.1763      50.7134 69.1063
    4       0.2821       4.3905     820.7424      51.0385 70.3896
    5       0.2841       4.2988     809.1566      49.1931 69.1112
    6       0.2983       4.0198     801.9614      44.1202 69.8704
    7       0.2969       3.9342     789.3654      42.4376 66.7066
    8       0.2057       3.7129     779.4517      35.6243 64.4760
    9       0.1873       3.5761     769.6764      32.7974 63.5488
   10       0.2154       3.4560     762.7556      31.4504 63.5180
   11       0.1937       3.2319     756.2703      27.3190 64.2392
   12       0.1664       2.8510     747.0734      21.1692 65.3016
   13       0.1236       2.6075     735.7861      17.2348 62.7755
   14       0.1618       2.5357     728.1040      17.0029 65.8014
   15       0.1427       2.5008     721.4021      16.2310 64.5349
   16       0.1414       2.4918     714.7147      16.1015 62.1869
   17       0.1515       2.2319     708.9310      13.3330 64.9643
   18       0.1084       2.3143     700.6647      13.5591 62.6090
   19       0.1085       2.0820     693.3052      11.1096 62.2923
   20       0.1272       2.1314     683.8339      11.8732 62.3731
   21       0.1386       2.2930     674.5453      13.7714 62.6216
   22       0.1330       2.4451     653.2690      15.4095 62.1207
   23       0.1051       2.8561     611.3138      20.2399 62.6942
   24       0.1559       2.5940     575.4464      17.6073 64.6332
   25       0.2121       2.8435     540.7662      22.1042 62.2129
   26       0.1232       2.5172     525.3016      16.1183 62.6209
   27       0.1395       2.4364     504.6867      15.4152 62.7172
   28       0.1660       2.5734     490.8397      17.5468 64.8366
   29       0.1466       2.2955     480.8054      13.9365 62.3256
   30       0.1345       2.3896     466.6364      14.7909 62.5205
   31       0.1359       2.4696     457.9928      15.7451 64.5630
   32       0.1428       2.7496     445.0329      19.3680 62.5350
   33       0.1434       2.6294     439.7210      17.8275 65.1675
   34       0.1467       2.8600     431.8276      20.9225 62.2530
   35       0.1704       3.2081     425.8728      26.4440 62.2690
   36       0.1646       2.9125     421.8273      21.9849 62.1694
   37       0.1400       2.6372     416.9889      17.8677 64.5893
   38       0.1325       2.4115     412.6876      15.0104 62.7851
   39       0.1275       2.6611     407.6593      17.9703 62.4212
   40       0.1304       2.6724     403.3351      18.1604 64.1901
   41       0.1271       2.4783     399.6783      15.7093 64.2505
   42       0.1310       2.2246     394.9388      12.9071 76.6940
   43       0.1423       2.1885     392.5384      12.7105 67.7998
   44       0.1362       2.1389     388.8090      12.0926 68.9318
   45       0.1245       1.9719     385.7982      10.2611 71.9172
   46       0.1093       1.9438     380.9122       9.7841 69.3523
   47       0.1289       2.0784     377.6177      11.3640 69.2167
   48       0.0977       1.7665     374.1982       8.0626 69.3717
   49       0.0958       1.6033     368.7710       6.7204 68.8605
   50       0.0773       1.4873     364.4605       5.6674 69.3810
   51       0.0997       1.4528     361.4711       5.6623 72.1844
   52       0.0817       1.4180     358.9965       5.2257 69.3368
   53       0.0809       1.3781     356.0505       4.9508 69.1236
   54       0.0910       1.3505     352.2426       4.8745 71.7423
   55       0.0711       1.2030     350.3768       3.7767 69.6465
   56       0.0576       1.1562     346.2278       3.4074 69.6172
   57       0.0576       1.0800     341.6084       2.9987 68.9927
   58       0.0602       1.0362     338.0813       2.7944 68.7471
   59       0.0618       1.0282     335.2234       2.7664 69.0701
   60       0.0640       0.9490     332.1252       2.4074 72.0172
   61       0.0657       0.8755     329.0105       2.0987 71.8017
   62       0.0503       0.8321     326.9677       1.8137 69.3632
   63       0.0485       0.8516     324.0194       1.8819 72.5366
   64       0.0443       0.8718     321.7296       1.9418 71.8825
   65       0.0454       0.9018     318.0738       2.0752 63.1293
   66       0.0496       0.8946     315.9973       2.0685 62.5842
   67       0.0413       0.9450     313.1878       2.2457 59.3705
   68       0.0446       0.9464     310.1714       2.2689 56.6790
   69       0.0536       0.9606     307.1899       2.3869 51.9582
   70       0.0524       0.9951     302.4933       2.5412 56.5389
   71       0.0597       1.0204     300.8116       2.7128 56.1494
   72       0.0934       0.9872     298.2088       2.8624 58.3530
   73       0.0894       1.0924     294.3122       3.3440 58.4122
   74       0.1178       1.0827     289.0367       3.6464 56.1820
   75       0.1134       1.1498     285.2241       3.9451 56.3292
   76       0.0978       1.1454     281.8725       3.7230 56.6181
   77       0.0996       1.2667     274.6933       4.4461 58.5951
   78       0.0990       1.1872     270.9799       3.9710 56.2170
   79       0.0826       1.0040     268.5894       2.8282 56.4323
   80       0.0897       0.9403     265.9025       2.6048 58.5295
   81       0.0752       0.9896     262.9224       2.6892 56.7085
   82       0.0561       1.0597     259.2075       2.8840 56.5355
   83       0.0585       1.0496     256.3057       2.8497 56.5424
   84       0.0526       1.0127     254.8903       2.6275 58.7713
   85       0.0475       0.9999     253.3501       2.5347 55.9481
   86       0.0380       1.1126     252.2208       3.0574 56.3362
   87       0.0434       1.0834     249.2778       2.9305 58.5791
   88       0.0461       1.1424     247.8081       3.2601 58.8896
   89       0.0520       1.1624     246.2819       3.4053 56.2813
   90       0.0433       1.1251     245.0811       3.1501 56.3306
   91       0.0434       1.0280     242.2276       2.6493 56.1499
   92       0.0427       0.9485     240.8824       2.2683 56.4195
   93       0.0382       0.9566     239.2392       2.2838 58.9151
   94       0.0362       0.9632     238.1462       2.3052 58.5519
   95       0.0373       0.9461     236.8401       2.2317 56.6061
   96       0.0518       0.9864     234.6872       2.4963 58.3212
   97       0.0587       1.0011     232.0020       2.6121 56.1783
   98       0.0586       1.0407     229.3525       2.8053 56.3219
   99       0.0433       1.0039     227.6010       2.5312 58.5189
  100       0.0626       0.9236     225.7769       2.2828 56.3719
  101       0.0766       0.9597     224.0355       2.5627 56.5731
  102       0.0562       0.9785     221.5858       2.4877 58.3982
  103       0.0522       0.9569     220.4409       2.3614 56.3188
  104       0.0556       0.9435     219.4425       2.3218 58.2213
  105       0.0620       0.9401     218.0519       2.3518 55.9068
  106       0.0512       0.9477     216.9170       2.3127 58.5918
  107       0.0499       0.9355     215.7925       2.2496 56.1509
  108       0.0509       0.9554     214.3406       2.3457 69.1827
  109       0.0522       0.9983     212.6881       2.5552 59.2614
  110       0.0543       1.0137     211.3856       2.6431 60.9806
  111       0.0507       1.0649     207.2421       2.8759 61.6287
  112       0.0611       1.1199     205.0054       3.2341 63.7407
  113       0.0733       1.1601     201.9928       3.5525 63.4965
  114       0.0730       1.1498     199.6068       3.4923 61.3570
  115       0.0642       1.1442     195.1678       3.3892 61.2869
  116       0.0764       1.1253     193.2191       3.3894 61.1933
  117       0.0823       1.2233     191.9511       3.9981 63.5728
  118       0.0680       1.3003     189.5755       4.3355 61.7408
  119       0.0832       1.3812     187.1611       4.9943 61.3955
  120       0.0876       1.3295     184.9888       4.7030 62.2713
  121       0.0908       1.3124     182.8044       4.6282 62.8421
  122       0.0895       1.1893     180.6424       3.8757 65.1075
  123       0.0901       1.1441     177.9941       3.6289 62.1244
  124       0.0868       1.1091     176.0339       3.4047 62.3736
  125       0.0863       1.1411     174.1190       3.5718 64.3270
  126       0.0872       1.1372     172.7099       3.5598 64.5437
  127       0.0889       1.1673     171.0303       3.7441 62.2045
  128       0.0845       1.2076     169.7316       3.9283 65.2978
  129       0.0818       1.1881     168.2996       3.7892 61.4201
  130       0.0857       1.1984     166.5868       3.8879 64.1883
  131       0.0795       1.2423     165.1214       4.0834 64.8666
  132       0.0702       1.2504     162.7851       4.0479 62.1868
  133       0.0636       1.3494     161.5062       4.6129 61.9764
  134       0.0674       1.3549     159.4428       4.6784 64.5958
  135       0.0693       1.3848     158.4463       4.8910 61.5086
  136       0.0657       1.3475     156.9385       4.6165 64.1484
  137       0.0715       1.4220     154.7968       5.1598 62.0345
  138       0.0782       1.4094     153.3439       5.1345 64.9683
  139       0.0802       1.3746     151.4914       4.9208 62.1531
  140       0.0957       1.2236     150.8336       4.1423 62.5356
  141       0.0753       1.1719     148.0123       3.6359 62.1562
  142       0.0666       1.1419     146.8188       3.3955 64.4557
  143       0.0789       1.1347     145.4228       3.4634 61.6244
  144       0.0765       1.1183     144.8661       3.3527 64.2047
  145       0.0762       1.1170     143.3993       3.3431 64.0837
  146       0.0838       1.1129     142.0007       3.3940 63.2144
  147       0.0799       1.1477     141.2308       3.5437 62.0550
  148       0.0782       1.1377     140.2795       3.4733 62.1953
  149       0.0797       1.1485     139.5370       3.5470 64.7272
  150       0.0830       1.1425     138.6334       3.5459 61.8579
  151       0.0864       1.1703     137.8645       3.7355 61.5918
  152       0.0879       1.1797     136.8684       3.8031 61.9844
  153       0.0725       1.1473     136.1268       3.4743 62.0276
  154       0.0761       1.1605     134.9793       3.5800 62.2615
  155       0.0678       1.1409     134.4413       3.3996 62.2773
  156       0.0692       1.1143     133.7001       3.2670 64.4703
  157       0.0621       1.1250     133.1897       3.2685 64.4073
  158       0.0590       1.1318     132.0539       3.2831 61.7881
  159       0.0602       1.1166     131.1966       3.2096 63.9839
  160       0.0569       1.1125     130.2534       3.1650 61.7663
  161       0.0530       1.1121     129.5266       3.1371 64.5078
  162       0.0522       1.1223     128.6091       3.1868 62.1417
  163       0.0587       1.1266     127.6438       3.2532 64.2660
  164       0.0603       1.1438     126.1749       3.3580 62.1537
  165       0.0554       1.1608     125.5061       3.4184 62.1205
  166       0.0540       1.1407     124.7082       3.2976 62.6513
  167       0.0589       1.1066     124.0229       3.1472 60.8130
  168       0.0539       1.1022     123.1722       3.0900 58.1917
  169       0.0580       1.0349     121.9509       2.7722 59.2248
  170       0.0645       1.0271     120.8793       2.7813 56.7841
  171       0.0616       1.0312     120.0356       2.7799 59.1764
  172       0.0595       1.0106     119.6892       2.6633 59.0367
  173       0.0612       1.0002     119.2210       2.6260 57.0040
  174       0.0616       0.9920     118.8105       2.5895 56.9247
  175       0.0606       0.9601     118.0022       2.4329 59.3183
  176       0.0611       0.9692     117.3999       2.4783 56.5070
  177       0.0647       0.9729     116.8218       2.5226 59.0607
  178       0.0621       0.9479     116.1815       2.3879 56.6396
  179       0.0647       0.9217     115.3140       2.2905 58.5109
  180       0.0682       0.9326     114.6472       2.3667 58.8137
  181       0.0712       0.9111     114.1768       2.2965 58.1328
  182       0.0676       0.9053     113.5548       2.2411 58.2210
  183       0.0723       0.9012     112.7674       2.2625 56.3776
  184       0.0739       0.8924     112.3620       2.2392 58.7011
  185       0.0752       0.8779     111.7120       2.1890 56.1438
  186       0.0725       0.8760     111.2808       2.1574 56.1695
  187       0.0653       0.8777     110.9720       2.1050 58.3943
  188       0.0698       0.8696     110.5967       2.1072 56.0237
  189       0.0639       0.8825     109.8473       2.1143 56.1443
  190       0.0620       0.8903     109.0376       2.1332 58.5452
  191       0.0567       0.8637     108.3827       1.9833 58.6487
  192       0.0566       0.8782     107.6153       2.0433 56.2479
  193       0.0661       0.8696     107.0402       2.0768 56.4169
  194       0.0626       0.8668     106.4078       2.0384 56.4901
  195       0.0644       0.8681     106.0588       2.0575 56.0183
  196       0.0659       0.8812     105.6684       2.1242 55.7473
  197       0.0620       0.8856     105.4327       2.1132 58.0191
  198       0.0603       0.8937     104.7315       2.1353 58.2622
  199       0.0603       0.8995     104.3453       2.1599 56.1174
  200       0.0572       0.8943     104.1153       2.1156 58.4399
...Training Complete!

