Fri Mar 13 07:07:17 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [0.972, 6.379, 0.477], 'O': [1.09, 8.575, 0.603], 'Cu': [2.168, 3.8386, 1.696]}
Optimizer results: 
 fun: 93.905415248483 
 message: Max. number of function evaluations reached 
 nfev: 100 
 nit: 25 
 success: False
Fitted LJ parameters: {'C': array([1.4277085 , 6.31000182, 1.03460978]), 'O': array([1.41766048, 8.3890207 , 1.        ]), 'Cu': array([2.2184137 , 3.83552667, 1.27301783])} 

Optimization time: 2095.3429493904114 

Filename: COCu_morse_new_200_5k_LJ_100_iter_3
Dataset size: 500
Target scaling: [8.841315589518457, -1043.2646190902499]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 500
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.9266       7.8946    8830.7100     167.5809 37.9559
    2       0.1107       5.7974    1197.1642      67.8322 39.6817
    3       0.1647       5.5681     751.8121      63.3639 39.2944
    4       0.2137       5.2308     710.4558      57.0042 39.4356
    5       0.2080       4.9961     688.5473      52.0857 39.2282
    6       0.1177       4.9522     676.9729      49.7411 37.9132
    7       0.1114       5.0268     661.3593      51.1566 37.7045
    8       0.1475       4.8467     654.9492      48.0690 37.7123
    9       0.1794       4.5857     645.7470      43.6655 39.1441
   10       0.1641       4.4810     640.5254      41.5040 37.9204
   11       0.1824       4.3741     636.1484      39.9283 37.7612
   12       0.1742       4.3803     627.0471      39.8911 37.8438
   13       0.2045       4.1263     619.2692      36.1450 37.9089
   14       0.1422       4.0057     608.9074      33.1036 39.1898
   15       0.1158       3.8294     601.6503      29.9991 37.9905
   16       0.1336       3.8925     589.0206      31.1965 37.7271
   17       0.1262       3.7828     578.2077      29.4160 37.8308
   18       0.1437       4.1498     564.0292      35.4743 37.9690
   19       0.1308       4.1966     537.0921      36.0790 33.2588
   20       0.1305       3.9197     501.0462      31.5795 39.2100
   21       0.1236       3.8515     487.7206      30.4313 37.7397
   22       0.1456       3.6285     475.2269      27.3915 39.0168
   23       0.1752       3.4013     459.2884      24.6723 38.0030
   24       0.1202       3.1713     451.8928      20.8376 37.8044
   25       0.1190       2.9485     443.8322      18.0956 37.9256
   26       0.1166       2.9287     437.2976      17.8341 37.7691
   27       0.1097       2.8028     431.9670      16.3128 39.2778
   28       0.1058       2.7670     423.8718      15.8722 37.7447
   29       0.0981       2.7281     416.0372      15.3665 37.9153
   30       0.1019       2.7546     411.7544      15.6952 37.7618
   31       0.0963       2.8029     408.1070      16.1763 37.8321
   32       0.1060       2.7647     403.9014      15.8488 37.7722
   33       0.1100       2.8226     401.6227      16.5392 37.5884
   34       0.1217       2.9457     398.0425      18.0951 37.6662
   35       0.1192       2.9088     393.7050      17.6335 39.2838
   36       0.1219       2.8050     391.4193      16.4788 37.8718
   37       0.1224       2.7751     389.3310      16.1514 39.2727
   38       0.1431       2.7151     386.9375      15.7671 39.0988
   39       0.1296       2.7070     382.7191      15.4960 39.0693
   40       0.1399       2.7129     380.4854      15.6981 37.8596
   41       0.1346       2.6841     377.7849      15.3142 37.8402
   42       0.1234       2.7268     375.0523      15.6325 37.6649
   43       0.1235       2.6444     372.0117      14.7477 37.9117
   44       0.1144       2.4483     369.9067      12.6421 37.7969
   45       0.1167       2.4020     364.8340      12.2203 39.4407
   46       0.1119       2.3334     362.7177      11.5152 39.2950
   47       0.1101       2.2497     360.4109      10.7283 37.6794
   48       0.1003       2.2766     355.9467      10.8692 39.1456
   49       0.0912       2.3336     354.1249      11.3078 37.5950
   50       0.0907       2.3718     352.5742      11.6619 39.2564
   51       0.0919       2.4307     350.8540      12.2394 37.7142
   52       0.0969       2.4033     346.4471      12.0221 37.6185
   53       0.1152       2.4732     344.5727      12.8977 37.7841
   54       0.1269       2.5059     341.0916      13.3635 39.1933
   55       0.1106       2.3875     338.5052      12.0111 37.4953
   56       0.1057       2.4158     335.8966      12.2314 39.3670
   57       0.1043       2.4163     334.4016      12.2206 37.8449
   58       0.0971       2.2546     332.1609      10.6383 38.0201
   59       0.0927       2.0335     330.2371       8.6995 39.2614
   60       0.0935       1.8878     326.9225       7.5646 39.4242
   61       0.0888       1.7504     324.3762       6.5222 37.8187
   62       0.0888       1.6715     321.7062       5.9821 39.1172
   63       0.0895       1.6376     319.7578       5.7639 37.9542
   64       0.0782       1.6035     318.0457       5.4484 39.4050
   65       0.0691       1.5639     315.5169       5.1302 39.3649
   66       0.0643       1.5831     313.4819       5.2192 37.6782
   67       0.0606       1.5439     311.0961       4.9511 37.8525
   68       0.0787       1.5689     309.8967       5.2330 38.1448
   69       0.1010       1.4411     306.1077       4.6632 39.2837
   70       0.1065       1.4445     302.8742       4.7406 37.7843
   71       0.0934       1.6215     301.5068       5.6947 38.0124
   72       0.0819       1.5852     298.8146       5.3610 32.2709
   73       0.0701       1.5141     297.5252       4.8307 38.2966
   74       0.0703       1.6404     296.1294       5.6292 39.8663
   75       0.0602       1.4367     293.1685       4.3095 38.2392
   76       0.0682       1.4585     290.2902       4.4874 39.7509
   77       0.0621       1.4472     289.0086       4.3814 38.3187
   78       0.0470       1.5092     284.9347       4.6658 38.1559
   79       0.0498       1.2797     281.1057       3.3992 39.7714
   80       0.0436       1.1598     278.8338       2.7852 38.3351
   81       0.0495       1.1450     276.8051       2.7444 38.3393
   82       0.0481       1.1245     275.6174       2.6447 38.1465
   83       0.0519       1.2628     274.2755       3.3241 39.6840
   84       0.0577       1.2237     273.0346       3.1615 39.9189
   85       0.0539       1.1942     271.8498       2.9975 39.7958
   86       0.0505       1.2197     271.0771       3.1032 39.8243
   87       0.0466       1.2917     270.2020       3.4452 39.7899
   88       0.0516       1.2187     268.5517       3.1033 38.3607
   89       0.0519       1.2302     267.6483       3.1618 38.2354
   90       0.0502       1.3005     266.6699       3.5088 39.6833
   91       0.0547       1.2259     265.7338       3.1556 39.6539
   92       0.0540       1.2149     264.9149       3.0981 39.6799
   93       0.0606       1.2184     264.3997       3.1526 38.3199
   94       0.0616       1.1711     262.8870       2.9326 39.8904
   95       0.0592       1.1423     262.0096       2.7852 39.8050
   96       0.0555       1.1206     261.5221       2.6653 39.5689
   97       0.0593       1.1648     260.8047       2.8890 38.2032
   98       0.0563       1.1715     260.4140       2.9035 38.3568
   99       0.0520       1.2342     259.8663       3.1819 39.6765
  100       0.0494       1.2458     259.2186       3.2262 39.6942
  101       0.0493       1.2488     258.7797       3.2406 38.2850
  102       0.0544       1.2420     258.2947       3.2329 38.2776
  103       0.0521       1.2017     257.7358       3.0240 38.3779
  104       0.0552       1.2155     257.0968       3.1076 38.1178
  105       0.0577       1.1348     256.7206       2.7422 38.4529
  106       0.0644       1.0637     256.1667       2.4702 38.4413
  107       0.0569       1.0530     255.6721       2.3794 38.1501
  108       0.0579       1.0136     255.2925       2.2224 38.1717
  109       0.0549       0.9864     254.9082       2.0969 38.2360
  110       0.0518       0.9705     254.4541       2.0178 38.3500
  111       0.0557       0.9695     254.0714       2.0354 38.3188
  112       0.0521       0.9293     253.7281       1.8627 39.8233
  113       0.0564       0.9371     253.0281       1.9151 38.1431
  114       0.0567       0.8864     251.5422       1.7323 39.6250
  115       0.0615       0.8969     250.3459       1.7978 39.9997
  116       0.0565       0.8874     249.5935       1.7346 38.3686
  117       0.0582       0.9418     248.6425       1.9431 39.8467
  118       0.0570       0.9463     247.6358       1.9538 38.1281
  119       0.0600       0.9232     247.2623       1.8847 38.2029
  120       0.0627       0.9572     246.7216       2.0291 39.6639
  121       0.0592       0.9545     246.1448       1.9976 37.8797
  122       0.0528       0.9537     245.7575       1.9582 37.9453
  123       0.0481       0.9434     245.2055       1.8957 39.7283
  124       0.0531       0.9621     244.7366       1.9918 38.0460
  125       0.0551       0.9491     244.3855       1.9530 38.0800
  126       0.0559       0.9023     244.0961       1.7845 38.0625
  127       0.0549       0.8993     243.6578       1.7682 38.1628
  128       0.0512       0.8860     243.4831       1.7012 38.0476
  129       0.0544       0.8879     243.0933       1.7250 39.5035
  130       0.0484       0.9134     242.7881       1.7855 38.0124
  131       0.0488       0.9681     242.6182       1.9938 39.5553
  132       0.0449       0.9993     242.3754       2.0980 39.5647
  133       0.0463       0.9410     242.1496       1.8782 39.6992
  134       0.0476       0.9568     241.8833       1.9443 38.1653
  135       0.0472       0.9549     241.5425       1.9350 39.5404
  136       0.0488       0.9490     241.4124       1.9204 38.2150
  137       0.0490       0.9677     241.0719       1.9928 39.4542
  138       0.0483       0.9395     240.8831       1.8820 38.2015
  139       0.0459       0.9226     240.3299       1.8077 38.0092
  140       0.0476       0.9006     240.2340       1.7355 37.9985
  141       0.0528       0.9039     240.1706       1.7735 39.6445
  142       0.0520       0.8922     239.7188       1.7274 38.3284
  143       0.0524       0.9074     239.3018       1.7840 39.8235
  144       0.0536       0.9035     239.1169       1.7764 39.4475
  145       0.0551       0.8797     238.8755       1.6995 39.6708
  146       0.0494       0.8598     238.7212       1.6006 38.0745
  147       0.0498       0.8206     238.5694       1.4709 38.2732
  148       0.0538       0.8172     238.0536       1.4806 39.8231
  149       0.0522       0.8234     237.5987       1.4923 39.7637
  150       0.0466       0.7534     237.2798       1.2439 39.4567
  151       0.0467       0.7590     236.6836       1.2612 38.0636
  152       0.0488       0.7654     236.5356       1.2908 38.0147
  153       0.0477       0.7534     236.3599       1.2489 38.2570
  154       0.0492       0.7619     235.8640       1.2822 36.6182
  155       0.0403       0.7528     235.6999       1.2148 38.0133
  156       0.0456       0.7713     235.1143       1.2935 38.1551
  157       0.0442       0.7620     234.8059       1.2589 38.2964
  158       0.0425       0.7685     234.4887       1.2712 39.7418
  159       0.0460       0.7015     234.0039       1.0900 38.0764
  160       0.0447       0.7659     233.3923       1.2729 39.6642
  161       0.0439       0.7693     232.6442       1.2800 38.0023
  162       0.0404       0.7720     232.3626       1.2737 39.4644
  163       0.0456       0.7855     232.1408       1.3380 38.2703
  164       0.0490       0.8026     231.8031       1.4084 38.2382
  165       0.0454       0.7452     231.3286       1.2134 38.0861
  166       0.0394       0.7728     230.7279       1.2721 38.3465
  167       0.0468       0.7156     230.1419       1.1340 38.3715
  168       0.0430       0.6873     229.5795       1.0372 38.3231
  169       0.0428       0.6860     229.3085       1.0328 38.4051
  170       0.0396       0.6747     229.1442       0.9888 39.8001
  171       0.0356       0.6866     229.0069       1.0062 39.7857
  172       0.0421       0.6933     228.7466       1.0502 39.6052
  173       0.0366       0.7015     228.2991       1.0512 39.7473
  174       0.0445       0.6576     227.8308       0.9639 39.6696
  175       0.0464       0.6682     227.5155       1.0006 38.2597
  176       0.0530       0.7882     226.9641       1.3831 38.0262
  177       0.0538       0.8035     225.7530       1.4360 39.8266
  178       0.0439       0.7981     224.6503       1.3706 39.8369
  179       0.0458       0.7769     223.6379       1.3122 39.7473
  180       0.0433       0.7763     222.6118       1.2989 38.3599
  181       0.0395       0.7876     222.0370       1.3187 38.1433
  182       0.0418       0.7606     221.7408       1.2441 38.3782
  183       0.0345       0.8018     221.4863       1.3453 38.2499
  184       0.0323       0.9003     220.9860       1.6733 38.1489
  185       0.0360       0.8694     219.8608       1.5765 38.4985
  186       0.0345       0.8863     219.5149       1.6306 39.6440
  187       0.0371       0.9081     219.2139       1.7183 39.7760
  188       0.0339       0.9277     218.7079       1.7788 38.1920
  189       0.0346       0.9476     217.6829       1.8554 39.6497
  190       0.0334       0.8635     216.5204       1.5472 38.3957
  191       0.0486       0.8988     216.0690       1.7338 39.8224
  192       0.0445       0.9723     215.2890       1.9899 39.7052
  193       0.0354       0.9750     214.4545       1.9638 38.3672
  194       0.0390       0.9259     213.2618       1.7907 39.7672
  195       0.0528       1.0101     211.9674       2.1804 39.7903
  196       0.0550       0.9493     210.7779       1.9537 38.3690
  197       0.0602       1.0860     209.3760       2.5399 38.3036
  198       0.0614       1.0492     207.0889       2.3906 39.6775
  199       0.0634       0.9770     206.4989       2.1101 38.3151
  200       0.0477       0.9856     206.0208       2.0565 38.4101
...Training Complete!

