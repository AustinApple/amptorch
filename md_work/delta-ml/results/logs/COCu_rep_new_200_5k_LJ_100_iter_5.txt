Fri Mar 13 16:29:32 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [0.972, 6.379], 'O': [1.09, 8.575], 'Cu': [2.168, 3.8386]}
Optimizer results: 
 fun: 7.571038729850903 
 message: Max. number of function evaluations reached 
 nfev: 100 
 nit: 17 
 success: False
Fitted LJ parameters: {'C': array([1.23832692, 0.02030579]), 'O': array([1.        , 0.20226578]), 'Cu': array([1.68483402, 0.69595634])} 

a: 5.407617131207893
Optimization time: 2299.889584302902 

Filename: COCu_rep_new_200_5k_LJ_100_iter_5
Dataset size: 700
Target scaling: [8.841315589518457, -56.5029366592828]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 700
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       1.9527       2.3681    8752.7451     282.6207 81.7302
    2       1.0331       5.2427    7071.4355     151.6786 81.1908
    3       0.2915       4.9320    6065.2856      74.0573 84.4451
    4       0.2663       3.9332    5440.6709      48.2799 102.5666
    5       0.3840       3.2648    4997.3916      40.1688 94.1291
    6       0.2331       2.9449    4669.9888      28.0877 93.0258
    7       0.1833       1.0894    4357.8452       5.6752 97.9342
    8       0.0748       1.5112    4059.3037       6.7864 93.7127
    9       0.1132       1.7972    3857.6223       9.9401 92.1684
   10       0.0967       1.5036    3742.7893       6.9853 96.9358
   11       0.0996       1.2147    3641.1785       4.8262 93.4590
   12       0.1051       1.0697    3555.9304       3.9765 94.4735
   13       0.1043       0.9396    3506.4231       3.2328 94.4115
   14       0.1286       0.8096    3477.2480       2.9927 94.3112
   15       0.1165       0.7864    3446.7512       2.6816 99.4299
   16       0.1272       0.8936    3422.4817       3.3678 94.8706
   17       0.1186       0.9225    3395.3542       3.3670 95.6404
   18       0.1116       1.0105    3373.0735       3.7309 96.2565
   19       0.1053       0.9196    3345.6831       3.1445 96.9020
   20       0.1115       0.8745    3334.6812       3.0116 97.1362
   21       0.1025       0.8561    3288.5249       2.7875 93.9243
   22       0.1117       0.8357    3266.6060       2.8293 93.4589
   23       0.0953       0.7779    3251.7661       2.3308 90.9302
   24       0.1036       0.9005    3235.5684       3.0218 82.1033
   25       0.0965       0.8676    3222.1501       2.7593 91.2036
   26       0.0970       0.9667    3211.7388       3.2761 80.2856
   27       0.0825       1.0686    3188.6152       3.6737 85.4683
   28       0.0975       1.1050    3147.1343       4.0845 82.8194
   29       0.0917       1.1200    3118.0774       4.1013 87.8847
   30       0.0873       1.2072    3106.4531       4.6144 85.8518
   31       0.1132       1.0515    3088.6609       3.9925 82.4220
   32       0.0989       1.1675    3067.9629       4.5011 83.8100
   33       0.1081       1.2301    3052.7109       5.0556 86.3815
   34       0.1166       1.1809    3028.7695       4.8563 80.4654
   35       0.0946       1.3541    2996.0835       5.7603 70.0062
   36       0.0709       1.3976    2948.5784       5.8210 77.4179
   37       0.1236       1.3013    2901.2649       5.8107 78.7391
   38       0.0946       1.3700    2829.4099       5.8819 70.2966
   39       0.0810       1.4695    2754.0813       6.5065 83.3006
   40       0.0957       1.3885    2718.3413       6.0390 75.5251
   41       0.1245       1.3422    2701.4692       6.1288 78.3105
   42       0.1614       1.3616    2682.9001       7.0158 75.2746
   43       0.1378       1.3305    2670.3848       6.2851 76.6302
   44       0.1427       1.3291    2657.4265       6.3728 79.0268
   45       0.1277       1.3468    2637.7690       6.2201 78.9029
   46       0.1316       1.3513    2635.4612       6.3244 75.6919
   47       0.1055       1.3840    2631.5747       6.1424 76.6433
   48       0.1082       1.3655    2628.7400       6.0407 75.5664
   49       0.0853       1.3973    2625.0076       5.9761 78.5805
   50       0.1125       1.3380    2621.2375       5.8989 75.6492
   51       0.1056       1.3337    2618.5774       5.7610 78.9255
   52       0.1199       1.3403    2617.0393       6.0355 78.0383
   53       0.1164       1.3526    2615.0952       6.0715 102.8696
   54       0.1282       1.3506    2611.8831       6.2589 87.4988
   55       0.1277       1.3481    2610.0857       6.2302 85.7179
   56       0.1024       1.3542    2608.5439       5.8691 89.4701
   57       0.1082       1.3565    2605.1118       5.9710 89.8526
   58       0.1044       1.3531    2602.5051       5.8895 95.1246
   59       0.0949       1.3546    2601.0598       5.7686 90.9527
   60       0.1006       1.3349    2593.7556       5.6982 94.1484
   61       0.0967       1.3265    2589.5945       5.5818 92.9321
   62       0.0922       1.3389    2587.5039       5.6144 91.3590
   63       0.0868       1.3506    2586.6812       5.6345 89.8175
   64       0.0865       1.3474    2585.5767       5.6074 90.7076
   65       0.0809       1.3653    2584.7407       5.6771 93.2429
   66       0.0895       1.3435    2583.5115       5.6150 94.2939
   67       0.1000       1.3129    2582.0684       5.5259 89.3655
   68       0.1003       1.3125    2579.3799       5.5273 90.5764
   69       0.1021       1.3122    2578.1025       5.5511 89.7516
   70       0.0883       1.3383    2577.7156       5.5605 91.7549
   71       0.0944       1.3351    2576.5754       5.6144 90.6499
   72       0.0923       1.3458    2573.3904       5.6672 91.2203
   73       0.0925       1.3576    2571.8103       5.7590 91.4261
   74       0.0927       1.3580    2570.5371       5.7653 52.2995
   75       0.0927       1.3580    2570.4812       5.7653 35.1823
   76       0.0927       1.3580    2570.4812       5.7653 35.1042
   77       0.0927       1.3580    2570.4812       5.7653 34.5610
   78       0.0927       1.3580    2570.4812       5.7653 35.2104
   79       0.0927       1.3580    2570.4812       5.7653 34.7343
   80       0.0927       1.3580    2570.4812       5.7653 35.0008
   81       0.0927       1.3580    2570.4812       5.7653 34.6113
   82       0.0927       1.3580    2570.4812       5.7653 35.3091
   83       0.0927       1.3580    2570.4812       5.7653 35.1437
   84       0.0927       1.3580    2570.4812       5.7653 35.2394
   85       0.0927       1.3580    2570.4812       5.7653 35.2641
   86       0.0927       1.3580    2570.4812       5.7653 35.0297
   87       0.0927       1.3580    2570.4812       5.7653 35.2175
   88       0.0927       1.3580    2570.4812       5.7653 35.1915
   89       0.0927       1.3580    2570.4812       5.7653 33.9456
   90       0.0927       1.3580    2570.4812       5.7653 35.0715
   91       0.0927       1.3580    2570.4812       5.7653 34.6831
   92       0.0927       1.3580    2570.4812       5.7653 33.8831
   93       0.0927       1.3580    2570.4812       5.7653 33.8142
   94       0.0927       1.3580    2570.4812       5.7653 35.1213
   95       0.0927       1.3580    2570.4812       5.7653 34.6506
   96       0.0927       1.3580    2570.4812       5.7653 34.9022
   97       0.0927       1.3580    2570.4812       5.7653 34.4463
   98       0.0927       1.3580    2570.4812       5.7653 35.4185
   99       0.0927       1.3580    2570.4812       5.7653 34.8318
  100       0.0927       1.3580    2570.4812       5.7653 34.9399
  101       0.0927       1.3580    2570.4812       5.7653 35.1987
  102       0.0927       1.3580    2570.4812       5.7653 34.9074
  103       0.0927       1.3580    2570.4812       5.7653 34.6985
  104       0.0927       1.3580    2570.4812       5.7653 34.1473
  105       0.0927       1.3580    2570.4812       5.7653 34.8509
  106       0.0927       1.3580    2570.4812       5.7653 34.7166
  107       0.0927       1.3580    2570.4812       5.7653 35.2700
  108       0.0927       1.3580    2570.4812       5.7653 34.3155
  109       0.0927       1.3580    2570.4812       5.7653 35.1315
  110       0.0927       1.3580    2570.4812       5.7653 34.8808
  111       0.0927       1.3580    2570.4812       5.7653 35.1127
  112       0.0927       1.3580    2570.4812       5.7653 34.5175
  113       0.0927       1.3580    2570.4812       5.7653 35.0212
  114       0.0927       1.3580    2570.4812       5.7653 34.7696
  115       0.0927       1.3580    2570.4812       5.7653 35.5697
  116       0.0927       1.3580    2570.4812       5.7653 35.1229
  117       0.0927       1.3580    2570.4812       5.7653 33.8312
  118       0.0927       1.3580    2570.4812       5.7653 34.2304
  119       0.0927       1.3580    2570.4812       5.7653 34.6935
  120       0.0927       1.3580    2570.4812       5.7653 34.3734
  121       0.0927       1.3580    2570.4812       5.7653 34.9310
  122       0.0927       1.3580    2570.4812       5.7653 33.6544
  123       0.0927       1.3580    2570.4812       5.7653 34.5890
  124       0.0927       1.3580    2570.4812       5.7653 34.3864
  125       0.0927       1.3580    2570.4812       5.7653 35.6223
  126       0.0927       1.3580    2570.4812       5.7653 35.0457
  127       0.0927       1.3580    2570.4812       5.7653 34.1002
  128       0.0927       1.3580    2570.4812       5.7653 34.8288
  129       0.0927       1.3580    2570.4812       5.7653 35.1245
  130       0.0927       1.3580    2570.4812       5.7653 34.1224
  131       0.0927       1.3580    2570.4812       5.7653 34.9236
  132       0.0927       1.3580    2570.4812       5.7653 34.2485
  133       0.0927       1.3580    2570.4812       5.7653 34.8341
  134       0.0927       1.3580    2570.4812       5.7653 34.4207
  135       0.0927       1.3580    2570.4812       5.7653 34.9346
  136       0.0927       1.3580    2570.4812       5.7653 35.7689
  137       0.0927       1.3580    2570.4812       5.7653 34.8283
  138       0.0927       1.3580    2570.4812       5.7653 34.1222
  139       0.0927       1.3580    2570.4812       5.7653 35.7084
  140       0.0927       1.3580    2570.4812       5.7653 34.1529
  141       0.0927       1.3580    2570.4812       5.7653 34.8777
  142       0.0927       1.3580    2570.4812       5.7653 34.0671
  143       0.0927       1.3580    2570.4812       5.7653 34.9781
  144       0.0927       1.3580    2570.4812       5.7653 33.5953
  145       0.0927       1.3580    2570.4812       5.7653 34.8041
  146       0.0927       1.3580    2570.4812       5.7653 34.0570
  147       0.0927       1.3580    2570.4812       5.7653 35.2998
  148       0.0927       1.3580    2570.4812       5.7653 34.0070
  149       0.0927       1.3580    2570.4812       5.7653 34.8463
  150       0.0927       1.3580    2570.4812       5.7653 35.3805
  151       0.0927       1.3580    2570.4812       5.7653 34.7411
  152       0.0927       1.3580    2570.4812       5.7653 34.8183
  153       0.0927       1.3580    2570.4812       5.7653 34.5584
  154       0.0927       1.3580    2570.4812       5.7653 34.0697
  155       0.0927       1.3580    2570.4812       5.7653 33.7188
  156       0.0927       1.3580    2570.4812       5.7653 34.9104
  157       0.0927       1.3580    2570.4812       5.7653 35.6339
  158       0.0927       1.3580    2570.4812       5.7653 34.7524
  159       0.0927       1.3580    2570.4812       5.7653 34.1593
  160       0.0927       1.3580    2570.4812       5.7653 35.4593
  161       0.0927       1.3580    2570.4812       5.7653 35.0909
  162       0.0927       1.3580    2570.4812       5.7653 35.2693
  163       0.0927       1.3580    2570.4812       5.7653 34.2663
  164       0.0927       1.3580    2570.4812       5.7653 35.5712
  165       0.0927       1.3580    2570.4812       5.7653 34.3850
  166       0.0927       1.3580    2570.4812       5.7653 34.8558
  167       0.0927       1.3580    2570.4812       5.7653 34.6908
  168       0.0927       1.3580    2570.4812       5.7653 36.0495
  169       0.0927       1.3580    2570.4812       5.7653 34.1786
  170       0.0927       1.3580    2570.4812       5.7653 34.7355
  171       0.0927       1.3580    2570.4812       5.7653 35.2342
  172       0.0927       1.3580    2570.4812       5.7653 35.3912
  173       0.0927       1.3580    2570.4812       5.7653 35.0085
  174       0.0927       1.3580    2570.4812       5.7653 34.8984
  175       0.0927       1.3580    2570.4812       5.7653 35.4162
  176       0.0927       1.3580    2570.4812       5.7653 33.5166
  177       0.0927       1.3580    2570.4812       5.7653 35.6029
  178       0.0927       1.3580    2570.4812       5.7653 34.6123
  179       0.0927       1.3580    2570.4812       5.7653 34.2147
  180       0.0927       1.3580    2570.4812       5.7653 35.2941
  181       0.0927       1.3580    2570.4812       5.7653 35.5992
  182       0.0927       1.3580    2570.4812       5.7653 34.2812
  183       0.0927       1.3580    2570.4812       5.7653 35.6402
  184       0.0927       1.3580    2570.4812       5.7653 33.6657
  185       0.0927       1.3580    2570.4812       5.7653 34.7393
  186       0.0927       1.3580    2570.4812       5.7653 34.7989
  187       0.0927       1.3580    2570.4812       5.7653 33.6868
  188       0.0927       1.3580    2570.4812       5.7653 34.8470
  189       0.0927       1.3580    2570.4812       5.7653 36.0015
  190       0.0927       1.3580    2570.4812       5.7653 34.3152
  191       0.0927       1.3580    2570.4812       5.7653 34.0825
  192       0.0927       1.3580    2570.4812       5.7653 34.4200
  193       0.0927       1.3580    2570.4812       5.7653 35.7847
  194       0.0927       1.3580    2570.4812       5.7653 34.4725
  195       0.0927       1.3580    2570.4812       5.7653 35.9642
  196       0.0927       1.3580    2570.4812       5.7653 34.5139
  197       0.0927       1.3580    2570.4812       5.7653 35.7959
  198       0.0927       1.3580    2570.4812       5.7653 34.2201
  199       0.0927       1.3580    2570.4812       5.7653 34.5958
  200       0.0927       1.3580    2570.4812       5.7653 35.0843
...Training Complete!

