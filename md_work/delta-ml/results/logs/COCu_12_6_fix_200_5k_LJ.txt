Filename: COCu_12_6_fix_200_5k_LJ
Dataset size: 200
Target scaling: [8.841315589518457, -175.40737909714886]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.adamw.AdamW'>
Learning Rate: 0.001
Batch Size: 25
Epochs: 1000
Shuffle: True
Train Split (k-fold if int, fraction if float): 5

Filename: COCu_12_6_fix_200_5k_LJ
Dataset size: 200
Target scaling: [8.841315589518457, -175.40737909714886]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 200
Epochs: 10
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.0085       0.7158      42.7245       0.4114 21.9772
    2       0.0108       0.7112       1.8183       0.4069 20.0530
    3       0.0107       0.6728       1.6624       0.3644 21.3767
    4       0.0077       0.6283       1.5301       0.3170 22.1430
    5       0.0131       0.5514       1.2498       0.2467 22.1274
    6       0.0270       0.5684       0.9446       0.2731 20.5682
    7       0.0133       0.5603       0.7779       0.2547 20.1655
    8       0.0054       0.5328       0.7061       0.2277 20.2030
    9       0.0106       0.5271       0.6508       0.2245 21.2172
   10       0.0084       0.5156       0.6143       0.2141 20.1987
...Training Complete!

Filename: COCu_12_6_fix_200_5k_LJ
Dataset size: 200
Target scaling: [8.841315589518457, -175.40737909714886]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 200
Epochs: 300
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.0158       0.7349      41.9712       0.4370 18.2016
    2       0.0096       0.6972       1.9568       0.3907 17.9717
    3       0.0341       0.6442       1.6310       0.3553 17.9680
    4       0.0187       0.6042       1.2934       0.2991 18.7153
    5       0.0129       0.5485       1.0349       0.2440 18.7197
    6       0.0154       0.5464       0.8789       0.2436 18.0087
    7       0.0109       0.5288       0.7408       0.2261 18.6776
    8       0.0067       0.5050       0.6272       0.2049 18.0159
    9       0.0112       0.4903       0.5730       0.1948 18.1228
   10       0.0056       0.4789       0.5380       0.1841 21.9606
   11       0.0102       0.4813       0.5156       0.1874 22.0290
   12       0.0087       0.4685       0.4938       0.1771 19.4366
   13       0.0064       0.4622       0.4733       0.1717 19.6949
   14       0.0091       0.4572       0.4552       0.1689 22.5658
   15       0.0076       0.4572       0.4363       0.1684 20.9188
   16       0.0079       0.4557       0.4255       0.1674 24.0277
   17       0.0051       0.4526       0.4116       0.1644 23.4072
   18       0.0056       0.4514       0.4058       0.1636 22.8066
   19       0.0054       0.4454       0.3960       0.1593 23.5222
   20       0.0052       0.4358       0.3873       0.1525 23.0041
   21       0.0047       0.4310       0.3757       0.1490 22.5689
   22       0.0047       0.4276       0.3716       0.1467 23.9048
   23       0.0049       0.4260       0.3644       0.1457 22.8834
   24       0.0046       0.4220       0.3591       0.1429 22.8565
   25       0.0049       0.4172       0.3551       0.1397 23.4773
   26       0.0060       0.4076       0.3459       0.1336 22.7309
   27       0.0047       0.4038       0.3399       0.1309 23.0317
   28       0.0044       0.3953       0.3339       0.1254 22.6431
   29       0.0060       0.3875       0.3289       0.1208 22.9465
   30       0.0075       0.3759       0.3217       0.1142 22.4838
   31       0.0047       0.3648       0.3100       0.1069 24.0366
   32       0.0049       0.3452       0.2947       0.0958 22.6081
   33       0.0057       0.3374       0.2819       0.0917 23.2721
   34       0.0100       0.3148       0.2747       0.0813 23.3218
   35       0.0092       0.3130       0.2603       0.0800 19.2125
   36       0.0089       0.2819       0.2491       0.0651 22.6412
   37       0.0039       0.2514       0.2326       0.0509 22.3757
   38       0.0039       0.2146       0.1985       0.0372 23.2064
   39       0.0070       0.2086       0.1723       0.0358 23.4816
   40       0.0075       0.1875       0.1635       0.0292 23.1047
   41       0.0069       0.1782       0.1460       0.0264 23.2609
   42       0.0035       0.1676       0.1328       0.0227 23.9820
   43       0.0039       0.1622       0.1188       0.0213 24.3895
   44       0.0034       0.1542       0.1120       0.0192 22.6639
   45       0.0039       0.1504       0.1021       0.0184 23.4642
   46       0.0039       0.1371       0.0970       0.0153 23.5675
   47       0.0029       0.1264       0.0912       0.0129 23.2319
   48       0.0034       0.1240       0.0875       0.0125 22.5221
   49       0.0024       0.1216       0.0838       0.0119 23.6165
   50       0.0027       0.1176       0.0810       0.0112 23.5829
   51       0.0021       0.1106       0.0786       0.0099 23.6347
   52       0.0026       0.1094       0.0757       0.0097 22.2727
   53       0.0020       0.1075       0.0739       0.0093 24.4283
   54       0.0031       0.1087       0.0723       0.0096 23.4795
   55       0.0025       0.1072       0.0708       0.0093 23.9867
   56       0.0032       0.1082       0.0689       0.0096 23.2177
   57       0.0027       0.1048       0.0676       0.0089 22.6302
   58       0.0029       0.1043       0.0658       0.0089 24.7526
   59       0.0031       0.1027       0.0646       0.0086 23.3693
   60       0.0030       0.1042       0.0625       0.0089 23.5275
   61       0.0024       0.1040       0.0615       0.0088 23.1630
   62       0.0029       0.1035       0.0601       0.0087 23.3273
   63       0.0027       0.1040       0.0592       0.0088 22.4819
   64       0.0025       0.1005       0.0584       0.0082 22.5804
   65       0.0029       0.0991       0.0571       0.0080 23.6238
   66       0.0028       0.0985       0.0564       0.0079 22.5304
   67       0.0019       0.0992       0.0557       0.0079 23.2020
   68       0.0023       0.1009       0.0548       0.0082 24.0476
   69       0.0025       0.1000       0.0541       0.0081 23.1546
   70       0.0014       0.0990       0.0534       0.0079 23.9347
   71       0.0015       0.0977       0.0522       0.0077 23.4995
   72       0.0017       0.0969       0.0512       0.0076 24.3151
   73       0.0018       0.0967       0.0505       0.0075 27.2768
   74       0.0021       0.0947       0.0494       0.0073 27.3214
   75       0.0019       0.0926       0.0487       0.0069 25.9944
   76       0.0014       0.0912       0.0482       0.0067 24.3230
   77       0.0012       0.0906       0.0475       0.0066 23.7854
   78       0.0009       0.0911       0.0468       0.0067 30.6365
   79       0.0009       0.0910       0.0464       0.0066 36.6771
   80       0.0009       0.0894       0.0457       0.0064 30.4397
   81       0.0011       0.0898       0.0447       0.0065 27.1500
   82       0.0015       0.0890       0.0440       0.0064 27.2578
   83       0.0014       0.0873       0.0435       0.0061 26.5309
   84       0.0012       0.0880       0.0427       0.0062 26.6322
   85       0.0012       0.0867       0.0422       0.0060 25.7208
   86       0.0010       0.0881       0.0415       0.0062 27.1018
   87       0.0009       0.0876       0.0407       0.0062 24.6349
   88       0.0011       0.0892       0.0403       0.0064 23.5502
   89       0.0011       0.0887       0.0396       0.0063 23.8038
   90       0.0008       0.0862       0.0390       0.0060 23.5736
   91       0.0009       0.0855       0.0382       0.0059 23.4249
   92       0.0008       0.0857       0.0375       0.0059 24.0596
   93       0.0008       0.0863       0.0370       0.0060 22.7313
   94       0.0009       0.0833       0.0368       0.0056 23.7255
   95       0.0009       0.0840       0.0360       0.0057 22.4834
   96       0.0009       0.0824       0.0355       0.0054 23.6758
   97       0.0009       0.0824       0.0352       0.0054 23.4290
   98       0.0010       0.0816       0.0347       0.0053 22.9763
   99       0.0012       0.0794       0.0341       0.0051 22.7830
  100       0.0011       0.0771       0.0336       0.0048 24.0726
  101       0.0011       0.0756       0.0329       0.0046 23.3943
  102       0.0009       0.0754       0.0318       0.0046 23.5030
  103       0.0009       0.0752       0.0316       0.0045 23.6578
  104       0.0011       0.0755       0.0310       0.0046 24.2821
  105       0.0009       0.0753       0.0303       0.0045 22.4988
  106       0.0010       0.0739       0.0299       0.0044 22.7795
  107       0.0010       0.0733       0.0295       0.0043 23.0209
  108       0.0010       0.0733       0.0289       0.0043 22.8532
  109       0.0010       0.0731       0.0283       0.0043 22.5666
  110       0.0011       0.0732       0.0279       0.0043 23.0696
  111       0.0011       0.0721       0.0275       0.0042 22.6996
  112       0.0015       0.0717       0.0272       0.0042 23.8086
  113       0.0009       0.0717       0.0268       0.0041 22.8812
  114       0.0012       0.0709       0.0265       0.0040 24.1258
  115       0.0013       0.0718       0.0261       0.0042 24.6973
  116       0.0010       0.0718       0.0258       0.0041 23.3505
  117       0.0010       0.0697       0.0253       0.0039 22.6624
  118       0.0009       0.0682       0.0247       0.0037 23.2742
  119       0.0008       0.0680       0.0244       0.0037 24.0106
  120       0.0007       0.0680       0.0241       0.0037 23.0250
  121       0.0008       0.0665       0.0237       0.0036 23.8082
  122       0.0007       0.0660       0.0234       0.0035 24.2683
  123       0.0006       0.0648       0.0232       0.0034 24.1792
  124       0.0006       0.0643       0.0230       0.0033 23.4122
  125       0.0008       0.0640       0.0228       0.0033 23.8435
  126       0.0007       0.0648       0.0225       0.0034 22.3728
  127       0.0008       0.0640       0.0223       0.0033 22.5678
  128       0.0007       0.0631       0.0219       0.0032 22.8611
  129       0.0009       0.0617       0.0217       0.0031 23.1694
  130       0.0008       0.0619       0.0214       0.0031 23.0213
  131       0.0008       0.0610       0.0211       0.0030 23.0690
  132       0.0007       0.0614       0.0208       0.0030 23.5425
  133       0.0006       0.0597       0.0205       0.0029 22.4632
  134       0.0005       0.0586       0.0203       0.0027 23.8244
  135       0.0005       0.0586       0.0201       0.0028 23.5685
  136       0.0005       0.0581       0.0198       0.0027 23.5153
  137       0.0005       0.0573       0.0197       0.0026 22.6916
  138       0.0007       0.0574       0.0194       0.0026 22.6682
  139       0.0006       0.0569       0.0192       0.0026 22.7534
  140       0.0007       0.0570       0.0190       0.0026 23.5963
  141       0.0007       0.0566       0.0189       0.0026 22.8927
  142       0.0006       0.0561       0.0187       0.0025 22.1959
  143       0.0008       0.0551       0.0186       0.0024 22.5416
  144       0.0010       0.0544       0.0184       0.0024 23.1141
  145       0.0009       0.0537       0.0182       0.0023 22.5145
  146       0.0006       0.0528       0.0181       0.0022 23.6120
  147       0.0006       0.0527       0.0180       0.0022 23.6350
  148       0.0006       0.0541       0.0178       0.0024 22.3458
  149       0.0006       0.0544       0.0176       0.0024 22.7755
  150       0.0006       0.0541       0.0174       0.0023 22.9783
  151       0.0006       0.0531       0.0173       0.0023 22.8877
  152       0.0006       0.0533       0.0170       0.0023 23.7134
  153       0.0007       0.0538       0.0169       0.0023 22.3383
  154       0.0007       0.0543       0.0167       0.0024 23.5823
  155       0.0007       0.0540       0.0165       0.0023 23.7010
  156       0.0007       0.0538       0.0163       0.0023 22.6570
  157       0.0009       0.0536       0.0162       0.0023 22.6447
  158       0.0007       0.0537       0.0161       0.0023 22.8430
  159       0.0008       0.0548       0.0159       0.0024 22.8991
  160       0.0007       0.0545       0.0157       0.0024 23.4907
  161       0.0007       0.0545       0.0156       0.0024 22.8805
  162       0.0006       0.0534       0.0155       0.0023 22.6629
  163       0.0006       0.0532       0.0154       0.0023 22.7736
  164       0.0006       0.0530       0.0153       0.0023 22.4329
  165       0.0005       0.0518       0.0151       0.0022 22.4407
  166       0.0005       0.0523       0.0149       0.0022 22.7249
  167       0.0006       0.0520       0.0148       0.0022 23.5860
  168       0.0007       0.0521       0.0147       0.0022 22.4390
  169       0.0007       0.0518       0.0146       0.0022 23.2256
  170       0.0007       0.0513       0.0144       0.0021 22.5535
  171       0.0007       0.0517       0.0143       0.0021 23.4478
  172       0.0007       0.0517       0.0141       0.0021 23.3785
  173       0.0007       0.0503       0.0140       0.0020 22.4789
  174       0.0008       0.0505       0.0139       0.0021 23.4250
  175       0.0009       0.0503       0.0139       0.0020 23.5825
  176       0.0008       0.0502       0.0138       0.0020 22.7816
  177       0.0009       0.0504       0.0137       0.0020 22.9347
  178       0.0008       0.0511       0.0135       0.0021 22.9805
  179       0.0009       0.0512       0.0134       0.0021 22.8070
  180       0.0009       0.0506       0.0133       0.0021 22.9620
  181       0.0009       0.0507       0.0132       0.0021 22.5370
  182       0.0008       0.0510       0.0132       0.0021 22.7121
  183       0.0009       0.0508       0.0130       0.0021 23.3472
  184       0.0009       0.0498       0.0129       0.0020 22.8245
  185       0.0009       0.0493       0.0128       0.0020 23.3437
  186       0.0009       0.0489       0.0127       0.0019 23.3143
  187       0.0009       0.0487       0.0126       0.0019 24.5298
  188       0.0009       0.0492       0.0125       0.0020 23.2121
  189       0.0009       0.0485       0.0124       0.0019 22.7988
  190       0.0009       0.0484       0.0123       0.0019 24.0663
  191       0.0008       0.0478       0.0123       0.0018 22.6474
  192       0.0009       0.0479       0.0122       0.0019 23.5898
  193       0.0009       0.0479       0.0121       0.0019 23.8553
  194       0.0009       0.0475       0.0120       0.0018 22.9619
  195       0.0009       0.0476       0.0119       0.0018 23.8313
  196       0.0009       0.0463       0.0119       0.0017 22.7813
  197       0.0007       0.0462       0.0118       0.0017 22.5335
  198       0.0006       0.0463       0.0117       0.0017 23.0323
  199       0.0007       0.0463       0.0117       0.0017 23.0110
  200       0.0006       0.0460       0.0116       0.0017 22.4588
  201       0.0007       0.0463       0.0115       0.0017 23.5728
  202       0.0007       0.0467       0.0115       0.0018 23.2461
  203       0.0006       0.0465       0.0114       0.0017 23.9660
  204       0.0006       0.0462       0.0113       0.0017 23.1117
  205       0.0006       0.0458       0.0112       0.0017 23.4847
  206       0.0007       0.0461       0.0111       0.0017 23.0503
  207       0.0006       0.0455       0.0111       0.0017 23.2483
  208       0.0006       0.0456       0.0110       0.0017 23.9278
  209       0.0006       0.0451       0.0110       0.0016 23.2461
  210       0.0006       0.0447       0.0109       0.0016 23.2998
  211       0.0006       0.0445       0.0109       0.0016 24.2140
  212       0.0007       0.0440       0.0109       0.0016 24.0235
  213       0.0006       0.0442       0.0108       0.0016 23.6247
  214       0.0006       0.0444       0.0107       0.0016 23.8255
  215       0.0006       0.0445       0.0107       0.0016 23.6203
  216       0.0006       0.0439       0.0107       0.0015 23.5267
  217       0.0005       0.0436       0.0106       0.0015 23.9220
  218       0.0005       0.0433       0.0106       0.0015 23.6020
  219       0.0005       0.0432       0.0105       0.0015 23.6072
  220       0.0006       0.0429       0.0104       0.0015 22.6265
  221       0.0006       0.0436       0.0104       0.0015 22.7850
  222       0.0006       0.0433       0.0104       0.0015 22.9935
  223       0.0006       0.0434       0.0103       0.0015 22.5532
  224       0.0006       0.0433       0.0103       0.0015 23.6450
  225       0.0007       0.0436       0.0102       0.0015 22.6603
  226       0.0007       0.0443       0.0102       0.0016 23.7960
  227       0.0007       0.0448       0.0102       0.0016 23.4743
  228       0.0006       0.0448       0.0101       0.0016 22.7234
  229       0.0007       0.0447       0.0101       0.0016 23.2893
  230       0.0006       0.0445       0.0100       0.0016 23.0537
  231       0.0006       0.0445       0.0099       0.0016 22.3366
  232       0.0006       0.0438       0.0099       0.0015 23.8820
  233       0.0008       0.0440       0.0098       0.0016 24.0205
  234       0.0006       0.0441       0.0098       0.0016 23.4798
  235       0.0006       0.0441       0.0097       0.0016 22.9725
  236       0.0006       0.0443       0.0097       0.0016 23.2376
  237       0.0006       0.0440       0.0097       0.0016 23.0868
  238       0.0006       0.0448       0.0096       0.0016 22.4822
  239       0.0007       0.0441       0.0096       0.0016 23.6420
  240       0.0008       0.0436       0.0096       0.0015 22.4476
  241       0.0007       0.0427       0.0095       0.0015 23.8547
  242       0.0006       0.0429       0.0095       0.0015 23.7585
  243       0.0006       0.0428       0.0094       0.0015 22.3975
  244       0.0006       0.0429       0.0094       0.0015 23.4406
  245       0.0007       0.0429       0.0093       0.0015 23.5665
  246       0.0006       0.0429       0.0093       0.0015 23.3641
  247       0.0006       0.0428       0.0093       0.0015 22.5657
  248       0.0005       0.0429       0.0092       0.0015 23.1841
  249       0.0006       0.0429       0.0092       0.0015 23.7240
  250       0.0006       0.0425       0.0091       0.0014 23.4631
  251       0.0006       0.0417       0.0091       0.0014 23.5082
  252       0.0007       0.0422       0.0091       0.0014 22.6846
  253       0.0007       0.0420       0.0090       0.0014 23.6402
  254       0.0006       0.0417       0.0090       0.0014 23.3997
  255       0.0008       0.0416       0.0090       0.0014 23.7928
  256       0.0007       0.0413       0.0089       0.0014 22.6499
  257       0.0007       0.0408       0.0089       0.0013 22.7595
  258       0.0008       0.0409       0.0088       0.0014 22.7339
  259       0.0008       0.0413       0.0088       0.0014 23.5469
  260       0.0008       0.0410       0.0088       0.0014 23.0016
  261       0.0007       0.0417       0.0087       0.0014 23.0365
  262       0.0007       0.0416       0.0087       0.0014 23.4084
  263       0.0008       0.0419       0.0087       0.0014 22.5840
  264       0.0008       0.0423       0.0086       0.0014 23.6451
  265       0.0007       0.0419       0.0086       0.0014 22.9646
  266       0.0008       0.0422       0.0086       0.0014 24.2722
  267       0.0007       0.0417       0.0086       0.0014 22.3582
  268       0.0006       0.0411       0.0085       0.0014 23.0920
  269       0.0007       0.0412       0.0085       0.0014 23.5789
  270       0.0007       0.0410       0.0084       0.0014 22.6127
  271       0.0007       0.0414       0.0084       0.0014 22.5532
  272       0.0006       0.0415       0.0084       0.0014 23.2558
  273       0.0007       0.0419       0.0083       0.0014 23.2353
  274       0.0007       0.0417       0.0083       0.0014 22.5499
  275       0.0009       0.0414       0.0083       0.0014 22.5256
  276       0.0008       0.0407       0.0083       0.0013 23.0951
  277       0.0009       0.0403       0.0082       0.0013 23.5838
  278       0.0011       0.0409       0.0082       0.0014 22.4940
  279       0.0009       0.0406       0.0082       0.0013 23.6474
  280       0.0008       0.0405       0.0082       0.0013 23.3787
  281       0.0009       0.0403       0.0081       0.0013 22.7854
  282       0.0008       0.0405       0.0081       0.0013 23.0318
  283       0.0008       0.0409       0.0081       0.0014 23.2314
  284       0.0008       0.0407       0.0080       0.0013 23.5399
  285       0.0008       0.0413       0.0080       0.0014 24.9282
  286       0.0009       0.0417       0.0080       0.0014 22.4198
  287       0.0008       0.0413       0.0080       0.0014 22.7780
  288       0.0008       0.0412       0.0079       0.0014 22.4434
  289       0.0008       0.0410       0.0079       0.0014 22.7350
  290       0.0007       0.0405       0.0079       0.0013 22.6883
  291       0.0008       0.0406       0.0079       0.0013 23.5023
  292       0.0007       0.0402       0.0079       0.0013 23.5758
  293       0.0008       0.0401       0.0078       0.0013 22.6641
  294       0.0007       0.0402       0.0078       0.0013 23.8351
  295       0.0006       0.0401       0.0078       0.0013 22.6258
  296       0.0006       0.0399       0.0077       0.0013 22.6074
  297       0.0007       0.0403       0.0077       0.0013 23.7906
  298       0.0006       0.0404       0.0077       0.0013 23.3289
  299       0.0007       0.0405       0.0076       0.0013 23.0199
  300       0.0006       0.0405       0.0076       0.0013 23.4188
...Training Complete!

