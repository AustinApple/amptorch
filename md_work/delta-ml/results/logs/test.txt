Tue Mar 10 08:40:59 2020
--------------------------------------------------
Filename: test
Dataset size: 200
Target scaling: [0, 0]
Symmetry function parameters:
     G2_etas: [0.05       0.23207944 1.07721735 5.        ]
     G2_rs_s: [0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 60
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 200
Epochs: 1
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.0024       0.2996       4.3156       0.0719 12.0417
...Training Complete!

