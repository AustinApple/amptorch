Fri Mar 13 03:58:28 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [0.972, 6.379, 0.477], 'O': [1.09, 8.575, 0.603], 'Cu': [2.168, 3.8386, 1.696]}
Optimizer results: 
 fun: 102.42723321690286 
 message: Max. number of function evaluations reached 
 nfev: 100 
 nit: 22 
 success: False
Fitted LJ parameters: {'C': array([1.31873215, 6.30723857, 1.        ]), 'O': array([1.32167731, 8.5629867 , 1.        ]), 'Cu': array([2.17429965, 3.7127266 , 1.16732622])} 

Optimization time: 1705.8961672782898 

Filename: COCu_morse_new_200_5k_LJ_100_iter_2
Dataset size: 400
Target scaling: [8.841315589518457, -1044.577960207924]
Symmetry function parameters:
     G2_etas: [0.05       0.09653489 0.18637969 0.35984284 0.69474775 1.3413479
 2.58973734 5.        ]
     G2_rs_s: [0, 0, 0, 0, 0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0, 6.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=96, out_features=30, bias=True)
        (1): Tanh()
        (2): Linear(in_features=30, out_features=30, bias=True)
        (3): Tanh()
        (4): Linear(in_features=30, out_features=30, bias=True)
        (5): Tanh()
        (6): Linear(in_features=30, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 96
   # of Hidden Layers - 3
   Nodes/Layer - 30
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 400
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.7890       7.3363    2535.7080     111.0136 35.2469
    2       0.4530       7.1305     913.3797      89.5589 34.9655
    3       0.3580       6.8638     729.2170      80.5049 34.9339
    4       0.1050       6.5660     658.3210      69.4216 35.0244
    5       0.1091       6.4470     610.8663      66.9788 35.2139
    6       0.1473       6.1474     598.9092      61.3330 33.8771
    7       0.1283       5.5612     581.2927      50.1416 33.8991
    8       0.1349       4.7892     567.3821      37.4264 35.3948
    9       0.1506       4.3744     544.9851      31.5245 33.9486
   10       0.1310       4.2078     531.3874      29.0151 33.8795
   11       0.1670       3.7076     525.2304      23.1096 33.9098
   12       0.1836       3.4135     509.4545      19.9923 35.1523
   13       0.1782       3.4282     495.9610      20.0746 34.1560
   14       0.1863       3.3731     488.7565      19.5919 35.4172
   15       0.1795       3.3061     482.4606      18.7780 35.3031
   16       0.1615       3.0905     475.5313      16.3252 34.0787
   17       0.1696       3.0088     471.1775      15.6349 35.0503
   18       0.1693       3.0005     467.1090      15.5510 35.2707
   19       0.1829       3.0391     465.2722      16.1167 34.3957
   20       0.1608       2.9780     462.8188      15.2240 35.5154
   21       0.1514       3.0507     459.9068      15.8080 35.2575
   22       0.1634       2.9836     457.4389      15.3114 35.2931
   23       0.1511       2.9828     455.4164      15.1486 33.9406
   24       0.1412       2.9610     452.5516      14.8258 34.1240
   25       0.1389       2.8966     451.1905      14.1965 35.1348
   26       0.1246       2.8387     449.2429      13.5148 33.9301
   27       0.1151       2.8674     446.4125      13.6845 33.9026
   28       0.1127       2.8957     445.4153      13.9244 33.8531
   29       0.1132       2.8627     444.8146      13.6246 34.2958
   30       0.0981       3.4077     443.8091      18.9643 34.3681
   31       0.0892       2.9483     442.0164      14.2255 34.1860
   32       0.0912       3.1683     440.8690      16.3940 35.3803
   33       0.0944       3.4935     439.4121      19.8834 35.3494
   34       0.0994       3.2632     438.3354      17.4324 35.4350
   35       0.0964       2.7814     437.4468      12.7502 34.3219
   36       0.1016       2.7065     435.4406      12.1333 35.2801
   37       0.1101       2.6789     433.3847      11.9669 33.8300
   38       0.0882       2.5604     431.3880      10.8002 29.0482
   39       0.0750       2.5098     427.2512      10.3033 33.8105
   40       0.1401       2.4869     422.5349      10.6805 34.0547
   41       0.1464       3.0743     414.2271      15.9796 34.0615
   42       0.1264       2.5033     399.3394      10.6661 30.1845
   43       0.1179       2.6820     314.0903      12.0644 31.4241
   44       0.1736       2.3861     276.6955      10.3154 33.9761
   45       0.1028       1.6180     252.4286       4.6115 35.2286
   46       0.0617       1.4926     237.7917       3.7167 34.1450
   47       0.0501       1.2105     223.8369       2.4450 34.1802
   48       0.0432       0.9798     216.8220       1.6107 35.4413
   49       0.0393       0.9645     210.6881       1.5502 35.3003
   50       0.0362       0.8612     207.9773       1.2391 35.1986
   51       0.0329       0.9316     204.3960       1.4320 35.3407
   52       0.0438       0.8570     201.1120       1.2517 33.8446
   53       0.0327       1.0021     197.7859       1.6497 34.0334
   54       0.0329       0.9668     195.8562       1.5389 34.0623
   55       0.0510       0.9906     193.2457       1.6739 35.4022
   56       0.0371       0.8703     190.5543       1.2668 34.4111
   57       0.0460       0.8711     189.1294       1.2987 34.0474
   58       0.0448       0.6902     188.5516       0.8425 35.3712
   59       0.0425       0.6389     186.7872       0.7254 35.2966
   60       0.0477       0.6002     185.9399       0.6673 34.0754
   61       0.0427       0.6636     185.2123       0.7774 35.1859
   62       0.0285       0.6860     184.5214       0.7855 33.9592
   63       0.0302       0.7289     183.6669       0.8866 33.9741
   64       0.0452       0.6672     183.3509       0.7940 33.7879
   65       0.0481       0.6420     182.4252       0.7520 35.1403
   66       0.0519       0.6394     181.7260       0.7620 35.2284
   67       0.0411       0.5670     181.4528       0.5818 35.3680
   68       0.0403       0.5424     180.8793       0.5359 33.9774
   69       0.0481       0.5483     180.4988       0.5735 34.2163
   70       0.0411       0.5588     179.8702       0.5673 33.9106
   71       0.0414       0.5600     179.4318       0.5704 34.2802
   72       0.0342       0.4877     179.1560       0.4274 35.2675
   73       0.0377       0.5047     178.8143       0.4644 35.6681
   74       0.0303       0.4822     178.4170       0.4088 33.9948
   75       0.0322       0.4925     177.9686       0.4295 33.8237
   76       0.0300       0.4992     177.0941       0.4347 33.7894
   77       0.0250       0.5021     176.9278       0.4284 35.1875
   78       0.0281       0.5102     176.7935       0.4480 34.1478
   79       0.0338       0.5116     176.5110       0.4646 33.7135
   80       0.0292       0.5020     176.2097       0.4373 35.2526
   81       0.0306       0.5091     175.8789       0.4521 34.0483
   82       0.0300       0.5054     175.7565       0.4446 34.3143
   83       0.0347       0.4847     175.6212       0.4242 33.8106
   84       0.0319       0.4785     175.3016       0.4070 33.9145
   85       0.0322       0.4778     175.0856       0.4068 34.3623
   86       0.0265       0.4919     174.8883       0.4153 33.7634
   87       0.0335       0.4593     174.4620       0.3822 35.4556
   88       0.0316       0.4483     174.3189       0.3615 33.7184
   89       0.0277       0.4272     174.0415       0.3226 35.1457
   90       0.0314       0.4578     173.8757       0.3747 34.0473
   91       0.0266       0.4743     173.5901       0.3881 34.1323
   92       0.0338       0.4566     173.2426       0.3794 34.1954
   93       0.0261       0.4572     173.0802       0.3619 35.4940
   94       0.0291       0.4668     172.8111       0.3825 35.3117
   95       0.0262       0.4799     172.5400       0.3958 33.9293
   96       0.0305       0.4584     172.4048       0.3735 35.1767
   97       0.0284       0.4873     171.8482       0.4123 33.8529
   98       0.0351       0.5041     171.3517       0.4558 35.2189
   99       0.0320       0.5056     171.1702       0.4499 35.5761
  100       0.0388       0.5491     171.0114       0.5426 33.8665
  101       0.0308       0.5112     170.5686       0.4561 35.2367
  102       0.0308       0.4674     170.1043       0.3875 32.5258
  103       0.0325       0.4448     169.6367       0.3588 34.2012
  104       0.0264       0.4245     169.0925       0.3162 33.7883
  105       0.0218       0.4399     168.7287       0.3287 35.5032
  106       0.0264       0.5079     168.4179       0.4406 34.3795
  107       0.0337       0.4634     168.0933       0.3890 34.2686
  108       0.0271       0.4404     167.8835       0.3398 35.1478
  109       0.0262       0.4383     167.6453       0.3349 35.1669
  110       0.0218       0.4165     167.3829       0.2966 33.7787
  111       0.0221       0.3908     166.9516       0.2638 34.0121
  112       0.0215       0.3753     166.7832       0.2439 34.0369
  113       0.0192       0.3508     166.6747       0.2116 35.2191
  114       0.0203       0.3410     166.5073       0.2025 33.9238
  115       0.0229       0.3434     166.2033       0.2096 33.6754
  116       0.0207       0.3382     166.0569       0.2000 33.9390
  117       0.0204       0.3431     165.8462       0.2051 35.1649
  118       0.0219       0.3572     165.7733       0.2233 34.1902
  119       0.0194       0.3219     165.5837       0.1809 33.9492
  120       0.0214       0.3097     165.3903       0.1717 33.8483
  121       0.0248       0.3341     165.2829       0.2032 33.8571
  122       0.0206       0.3206     165.1255       0.1814 34.3229
  123       0.0239       0.3507     164.8907       0.2196 35.3715
  124       0.0206       0.3226     164.6713       0.1836 34.1757
  125       0.0196       0.3066     164.4510       0.1658 33.8784
  126       0.0223       0.2908     164.3376       0.1552 33.9943
  127       0.0198       0.2974     164.1999       0.1571 35.1181
  128       0.0192       0.2816     164.1186       0.1417 34.1503
  129       0.0201       0.2760     164.0378       0.1381 33.8045
  130       0.0240       0.2921     163.9841       0.1595 33.8808
  131       0.0251       0.2963     163.8893       0.1656 35.1998
  132       0.0231       0.3018     163.7974       0.1671 33.8692
  133       0.0211       0.3030     163.7506       0.1647 35.4487
  134       0.0280       0.2915     163.6762       0.1675 34.1939
  135       0.0248       0.2975     163.6045       0.1662 33.9018
  136       0.0274       0.3011     163.5307       0.1751 35.4846
  137       0.0267       0.3128     163.4969       0.1851 33.9657
  138       0.0244       0.2956     163.4141       0.1636 33.9341
  139       0.0270       0.2911     163.3362       0.1647 34.1407
  140       0.0252       0.2977     163.2623       0.1672 34.1734
  141       0.0293       0.3087     163.1877       0.1868 33.9267
  142       0.0233       0.3124     163.1330       0.1779 33.8055
  143       0.0266       0.3267     163.0919       0.1990 35.5987
  144       0.0236       0.3278     163.0542       0.1942 35.0656
  145       0.0231       0.3098     162.9794       0.1750 34.0906
  146       0.0259       0.3202     162.9405       0.1908 34.2462
  147       0.0207       0.3176     162.8969       0.1785 35.5455
  148       0.0217       0.3174     162.8155       0.1799 34.3913
  149       0.0216       0.3142     162.7916       0.1766 35.3405
  150       0.0240       0.3246     162.7732       0.1916 34.0612
  151       0.0218       0.3108     162.7027       0.1735 35.3393
  152       0.0197       0.3090     162.6025       0.1683 35.4434
  153       0.0202       0.3077     162.4368       0.1678 34.2952
  154       0.0229       0.3301     162.2855       0.1954 35.6147
  155       0.0210       0.3253     162.1844       0.1870 34.0578
  156       0.0210       0.3157     162.1317       0.1772 34.3539
  157       0.0244       0.3350     162.0884       0.2035 34.1515
  158       0.0198       0.3356     161.9494       0.1959 35.2551
  159       0.0198       0.3271     161.7675       0.1869 35.3937
  160       0.0189       0.3155     161.6996       0.1736 35.4832
  161       0.0194       0.3300     161.6373       0.1892 34.3689
  162       0.0191       0.3130     161.5675       0.1713 33.9003
  163       0.0214       0.3051     161.5171       0.1672 35.3018
  164       0.0196       0.3037     161.4575       0.1629 34.4035
  165       0.0216       0.3135     161.3255       0.1759 33.9829
  166       0.0202       0.3124     161.1406       0.1724 34.3170
  167       0.0185       0.3140     161.0601       0.1715 35.4987
  168       0.0191       0.3022     161.0040       0.1607 33.9487
  169       0.0215       0.2996     160.9064       0.1620 35.1594
  170       0.0218       0.2859     160.8080       0.1498 34.4331
  171       0.0229       0.2922     160.7088       0.1575 34.0449
  172       0.0231       0.2894     160.6407       0.1554 34.3192
  173       0.0203       0.2920     160.5871       0.1529 35.2710
  174       0.0179       0.2931     160.4288       0.1502 35.1509
  175       0.0172       0.3086     160.3674       0.1642 34.2389
  176       0.0180       0.3076     160.2949       0.1643 35.1040
  177       0.0177       0.3088     160.2301       0.1651 34.0108
  178       0.0198       0.3082     160.2094       0.1676 34.0988
  179       0.0195       0.3000     160.1778       0.1592 33.8148
  180       0.0185       0.3067     160.1190       0.1642 35.5622
  181       0.0168       0.3178     160.0294       0.1728 33.7941
  182       0.0164       0.3162     159.9780       0.1707 35.1295
  183       0.0168       0.3223     159.9522       0.1776 34.1779
  184       0.0169       0.3437     159.9228       0.2004 35.1826
  185       0.0175       0.3453     159.8782       0.2030 35.5487
  186       0.0176       0.3511     159.8139       0.2096 33.9042
  187       0.0167       0.3424     159.7688       0.1988 35.2555
  188       0.0169       0.3368     159.7060       0.1928 34.3340
  189       0.0165       0.3523     159.6245       0.2095 33.8207
  190       0.0171       0.3390     159.5536       0.1956 34.0210
  191       0.0161       0.3324     159.5016       0.1872 33.7631
  192       0.0168       0.3296     159.4413       0.1851 34.3948
  193       0.0170       0.3268     159.3685       0.1824 33.9706
  194       0.0189       0.3313     159.2819       0.1899 35.2886
  195       0.0184       0.3218     159.1553       0.1792 35.5284
  196       0.0176       0.3160     159.0804       0.1721 34.1273
  197       0.0192       0.3177     159.0464       0.1762 33.6155
  198       0.0188       0.3173     159.0074       0.1752 32.8684
  199       0.0194       0.3175     158.9061       0.1764 34.2072
  200       0.0195       0.3198     158.8217       0.1788 33.0960
...Training Complete!

