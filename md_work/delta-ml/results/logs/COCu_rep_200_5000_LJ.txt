Wed Mar 11 19:28:42 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [1.0, 6.3535], 'O': [1.0808, 8.5357], 'Cu': [2.1717, 3.7575]}
Optimizer results: 
 fun: 0.08081581688553366 
 message: Local minimum reached (|pg| ~= 0) 
 nfev: 25 
 nit: 6 
 success: True
Fitted LJ parameters: {'C': array([1.e+00, 1.e-05]), 'O': array([1.e+00, 1.e-05]), 'Cu': array([1.        , 3.21489957])} 

a: 15.0
Optimization time: 143.55710792541504 

Filename: COCu_rep_200_5000_LJ
Dataset size: 200
Target scaling: [8.841315589518457, 7.198266847248682e-07]
Symmetry function parameters:
     G2_etas: [0.05       0.23207944 1.07721735 5.        ]
     G2_rs_s: [0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 60
   # of Hidden Layers - 2
   Nodes/Layer - 20
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 200
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

Training initiated...
Epoch   EnergyRMSE    ForceRMSE    TrainLoss    ValidLoss     Dur
===== ============ ============ ============ ============ =======
    1       0.0039       0.3006       2.9446       0.0726 12.0066
    2       0.0067       0.2941       0.2439       0.0701 12.3533
    3       0.0052       0.2898       0.2197       0.0677 11.8069
    4       0.0055       0.2901       0.1995       0.0680 11.6558
    5       0.0043       0.2792       0.1794       0.0627 12.2627
    6       0.0046       0.2665       0.1615       0.0572 12.4630
    7       0.0025       0.2683       0.1488       0.0577 12.0988
    8       0.0020       0.2675       0.1420       0.0573 12.2143
    9       0.0021       0.2664       0.1404       0.0569 11.6316
   10       0.0018       0.2653       0.1380       0.0564 12.6265
   11       0.0018       0.2653       0.1370       0.0564 12.2373
   12       0.0019       0.2648       0.1348       0.0562 12.1659
   13       0.0026       0.2655       0.1317       0.0565 11.7074
   14       0.0028       0.2649       0.1304       0.0563 12.3307
   15       0.0027       0.2660       0.1287       0.0568 12.1741
   16       0.0027       0.2652       0.1267       0.0564 12.2736
   17       0.0020       0.2623       0.1236       0.0551 12.1802
   18       0.0020       0.2613       0.1211       0.0547 12.6134
   19       0.0020       0.2606       0.1188       0.0544 12.5679
   20       0.0022       0.2589       0.1175       0.0537 12.3360
   21       0.0024       0.2574       0.1160       0.0531 11.9502
   22       0.0023       0.2555       0.1147       0.0523 11.8600
   23       0.0020       0.2535       0.1138       0.0515 11.7046
   24       0.0020       0.2519       0.1121       0.0508 12.3721
   25       0.0017       0.2508       0.1110       0.0504 11.7534
   26       0.0018       0.2498       0.1105       0.0500 12.2234
   27       0.0016       0.2473       0.1096       0.0490 12.1235
   28       0.0018       0.2464       0.1086       0.0486 11.7881
   29       0.0025       0.2446       0.1078       0.0480 11.6792
   30       0.0015       0.2406       0.1066       0.0463 12.1386
   31       0.0016       0.2390       0.1050       0.0458 11.6949
   32       0.0014       0.2335       0.1040       0.0437 11.9899
   33       0.0016       0.2270       0.1026       0.0413 11.8381
   34       0.0013       0.2106       0.1001       0.0355 11.7720
   35       0.0019       0.2053       0.0968       0.0338 11.8024
   36       0.0020       0.1951       0.0932       0.0305 11.7130
   37       0.0022       0.1802       0.0903       0.0261 12.2180
   38       0.0027       0.1702       0.0860       0.0233 12.1277
   39       0.0046       0.1481       0.0812       0.0180 12.1941
   40       0.0053       0.1321       0.0748       0.0145 12.2110
   41       0.0035       0.1228       0.0672       0.0123 12.2780
   42       0.0038       0.1225       0.0584       0.0123 11.7175
   43       0.0034       0.1137       0.0559       0.0106 12.2622
   44       0.0052       0.1105       0.0489       0.0103 12.2248
   45       0.0063       0.1087       0.0439       0.0102 11.8293
   46       0.0054       0.1024       0.0411       0.0090 12.0496
   47       0.0033       0.0948       0.0391       0.0074 12.0734
   48       0.0027       0.0898       0.0357       0.0066 11.6428
   49       0.0026       0.0861       0.0337       0.0061 11.6988
   50       0.0027       0.0846       0.0323       0.0059 11.6289
   51       0.0028       0.0823       0.0309       0.0056 11.8177
   52       0.0023       0.0804       0.0290       0.0053 11.6100
   53       0.0020       0.0786       0.0281       0.0050 12.2154
   54       0.0023       0.0767       0.0273       0.0048 11.5974
   55       0.0021       0.0758       0.0265       0.0047 12.1634
   56       0.0021       0.0737       0.0254       0.0044 11.6104
   57       0.0022       0.0702       0.0244       0.0040 12.4912
   58       0.0019       0.0692       0.0234       0.0039 11.6285
   59       0.0022       0.0698       0.0231       0.0040 11.7389
   60       0.0015       0.0698       0.0225       0.0039 12.0311
   61       0.0015       0.0696       0.0220       0.0039 12.1588
   62       0.0015       0.0692       0.0213       0.0039 11.7003
   63       0.0017       0.0665       0.0209       0.0036 11.7102
   64       0.0020       0.0664       0.0203       0.0036 12.0894
   65       0.0016       0.0664       0.0198       0.0036 11.6881
   66       0.0015       0.0629       0.0195       0.0032 11.6692
   67       0.0014       0.0625       0.0190       0.0032 11.8288
   68       0.0012       0.0628       0.0188       0.0032 12.0992
   69       0.0012       0.0628       0.0186       0.0032 12.2253
   70       0.0010       0.0629       0.0184       0.0032 11.6982
   71       0.0009       0.0612       0.0182       0.0030 11.7656
   72       0.0011       0.0622       0.0179       0.0031 11.7658
   73       0.0009       0.0623       0.0177       0.0031 11.8823
   74       0.0009       0.0619       0.0174       0.0031 12.2694
   75       0.0009       0.0611       0.0171       0.0030 12.1007
   76       0.0009       0.0607       0.0170       0.0030 11.8800
   77       0.0010       0.0611       0.0169       0.0030 12.1079
   78       0.0010       0.0600       0.0168       0.0029 12.1902
   79       0.0011       0.0582       0.0166       0.0027 12.1647
   80       0.0010       0.0556       0.0163       0.0025 11.7940
   81       0.0008       0.0554       0.0161       0.0025 11.8741
   82       0.0008       0.0550       0.0159       0.0024 11.6773
   83       0.0008       0.0549       0.0157       0.0024 12.4494
   84       0.0007       0.0548       0.0154       0.0024 12.2473
   85       0.0007       0.0558       0.0153       0.0025 12.3300
   86       0.0007       0.0555       0.0151       0.0025 11.6398
   87       0.0007       0.0553       0.0150       0.0025 12.3496
   88       0.0007       0.0542       0.0148       0.0024 11.8765
   89       0.0007       0.0539       0.0147       0.0023 12.1351
   90       0.0006       0.0542       0.0145       0.0024 11.6992
   91       0.0006       0.0537       0.0144       0.0023 12.1025
   92       0.0006       0.0536       0.0142       0.0023 12.1645
   93       0.0005       0.0537       0.0140       0.0023 11.6243
   94       0.0006       0.0547       0.0138       0.0024 12.3504
   95       0.0008       0.0553       0.0137       0.0025 11.7504
   96       0.0008       0.0554       0.0135       0.0025 11.5911
   97       0.0006       0.0550       0.0133       0.0024 11.7563
   98       0.0007       0.0541       0.0132       0.0023 11.6140
   99       0.0006       0.0531       0.0131       0.0023 11.6731
  100       0.0006       0.0518       0.0129       0.0022 11.7920
  101       0.0006       0.0509       0.0128       0.0021 11.7393
  102       0.0006       0.0506       0.0125       0.0021 12.1289
  103       0.0007       0.0503       0.0123       0.0020 12.3471
  104       0.0008       0.0494       0.0122       0.0020 11.7005
  105       0.0007       0.0492       0.0121       0.0019 11.8637
  106       0.0007       0.0477       0.0119       0.0018 11.6384
  107       0.0009       0.0477       0.0117       0.0018 11.8929
  108       0.0009       0.0478       0.0115       0.0018 11.7696
  109       0.0010       0.0476       0.0113       0.0018 12.3711
  110       0.0011       0.0481       0.0112       0.0019 12.2431
  111       0.0009       0.0462       0.0111       0.0017 11.6704
  112       0.0008       0.0460       0.0110       0.0017 12.2938
  113       0.0009       0.0461       0.0108       0.0017 12.1430
  114       0.0009       0.0463       0.0106       0.0017 11.8982
  115       0.0009       0.0462       0.0105       0.0017 11.7708
  116       0.0008       0.0454       0.0104       0.0017 11.7190
  117       0.0008       0.0458       0.0101       0.0017 11.6993
  118       0.0008       0.0452       0.0100       0.0016 11.6904
  119       0.0008       0.0450       0.0100       0.0016 12.1714
  120       0.0008       0.0441       0.0099       0.0016 12.2418
  121       0.0008       0.0440       0.0098       0.0016 12.2293
  122       0.0008       0.0440       0.0097       0.0016 11.8880
  123       0.0008       0.0435       0.0097       0.0015 12.3952
  124       0.0008       0.0433       0.0096       0.0015 12.0490
  125       0.0007       0.0430       0.0096       0.0015 11.8785
  126       0.0006       0.0425       0.0095       0.0014 12.4357
  127       0.0006       0.0428       0.0094       0.0015 11.8586
  128       0.0006       0.0421       0.0093       0.0014 12.6456
  129       0.0006       0.0416       0.0092       0.0014 11.9770
  130       0.0007       0.0413       0.0091       0.0014 11.7828
  131       0.0007       0.0409       0.0091       0.0014 12.3999
  132       0.0006       0.0417       0.0090       0.0014 12.4689
  133       0.0006       0.0423       0.0089       0.0014 12.0453
  134       0.0005       0.0418       0.0089       0.0014 11.8515
  135       0.0007       0.0422       0.0088       0.0014 12.3867
  136       0.0007       0.0422       0.0088       0.0014 12.5058
  137       0.0006       0.0422       0.0087       0.0014 13.1713
  138       0.0006       0.0419       0.0086       0.0014 12.4624
  139       0.0006       0.0423       0.0086       0.0014 11.8301
  140       0.0005       0.0425       0.0085       0.0014 11.8667
  141       0.0005       0.0420       0.0085       0.0014 12.2561
  142       0.0005       0.0417       0.0084       0.0014 11.7264
  143       0.0006       0.0416       0.0084       0.0014 11.7796
  144       0.0005       0.0417       0.0083       0.0014 11.8283
  145       0.0005       0.0414       0.0083       0.0014 11.9162
  146       0.0005       0.0415       0.0083       0.0014 12.2348
  147       0.0005       0.0421       0.0082       0.0014 12.3703
  148       0.0005       0.0420       0.0082       0.0014 11.9019
  149       0.0005       0.0420       0.0082       0.0014 11.8439
  150       0.0005       0.0424       0.0081       0.0014 12.5765
  151       0.0005       0.0426       0.0081       0.0015 12.5415
  152       0.0006       0.0425       0.0080       0.0015 11.7796
  153       0.0006       0.0427       0.0080       0.0015 11.7785
  154       0.0006       0.0424       0.0080       0.0014 11.7699
  155       0.0006       0.0416       0.0079       0.0014 11.8079
  156       0.0007       0.0414       0.0079       0.0014 11.7253
  157       0.0006       0.0414       0.0078       0.0014 11.8977
  158       0.0006       0.0411       0.0078       0.0014 12.3085
  159       0.0007       0.0410       0.0077       0.0014 11.7917
  160       0.0007       0.0412       0.0077       0.0014 11.8166
  161       0.0006       0.0415       0.0076       0.0014 12.4482
  162       0.0006       0.0412       0.0076       0.0014 12.1895
  163       0.0007       0.0416       0.0076       0.0014 11.8707
  164       0.0007       0.0414       0.0075       0.0014 11.8554
  165       0.0007       0.0418       0.0074       0.0014 12.0318
  166       0.0007       0.0409       0.0074       0.0014 11.9571
  167       0.0007       0.0406       0.0073       0.0013 12.4817
  168       0.0007       0.0404       0.0072       0.0013 11.8031
  169       0.0008       0.0404       0.0072       0.0013 11.8450
  170       0.0008       0.0404       0.0071       0.0013 12.2063
  171       0.0008       0.0404       0.0071       0.0013 11.8777
  172       0.0008       0.0406       0.0071       0.0013 11.7071
  173       0.0008       0.0403       0.0070       0.0013 12.1714
  174       0.0009       0.0406       0.0070       0.0013 12.3238
  175       0.0007       0.0403       0.0069       0.0013 11.7551
  176       0.0007       0.0403       0.0068       0.0013 11.9334
  177       0.0007       0.0396       0.0068       0.0013 11.8693
  178       0.0006       0.0392       0.0068       0.0012 11.7287
  179       0.0006       0.0392       0.0067       0.0012 12.3024
  180       0.0006       0.0394       0.0067       0.0012 12.2029
  181       0.0007       0.0392       0.0066       0.0012 12.2881
  182       0.0007       0.0386       0.0065       0.0012 12.4456
  183       0.0006       0.0381       0.0065       0.0012 12.2191
  184       0.0007       0.0376       0.0065       0.0011 12.3049
  185       0.0006       0.0374       0.0064       0.0011 12.2524
  186       0.0006       0.0366       0.0064       0.0011 12.2461
  187       0.0005       0.0362       0.0063       0.0011 12.6185
  188       0.0005       0.0358       0.0063       0.0010 11.8449
  189       0.0005       0.0354       0.0063       0.0010 11.6736
  190       0.0005       0.0355       0.0062       0.0010 11.6561
  191       0.0006       0.0356       0.0062       0.0010 12.0517
  192       0.0006       0.0355       0.0062       0.0010 12.0845
  193       0.0006       0.0358       0.0061       0.0010 12.0119
  194       0.0005       0.0359       0.0061       0.0010 11.9316
  195       0.0005       0.0360       0.0061       0.0010 12.0025
  196       0.0005       0.0359       0.0061       0.0010 12.0288
  197       0.0005       0.0361       0.0060       0.0010 11.5162
  198       0.0005       0.0360       0.0060       0.0010 11.9719
  199       0.0006       0.0365       0.0060       0.0011 12.0348
  200       0.0006       0.0367       0.0059       0.0011 11.9719
...Training Complete!

Thu Mar 12 15:31:40 2020
--------------------------------------------------
LJ-Parameter Optimization
inital LJ parameter guess [sig, eps]: {'C': [0.972, 6.379], 'O': [1.09, 8.575], 'Cu': [2.168, 3.8386]}
Optimizer results: 
 fun: 0.08081581670103682 
 message: Local minimum reached (|pg| ~= 0) 
 nfev: 16 
 nit: 5 
 success: True
Fitted LJ parameters: {'C': array([1.e+00, 1.e-05]), 'O': array([1.e+00, 1.e-05]), 'Cu': array([1.       , 3.0955202])} 

a: 15.0
Optimization time: 89.59337830543518 

Filename: COCu_rep_200_5000_LJ
Dataset size: 200
Target scaling: [8.841315589518457, 7.190565565443904e-07]
Symmetry function parameters:
     G2_etas: [0.05       0.23207944 1.07721735 5.        ]
     G2_rs_s: [0, 0, 0, 0]
     G4_etas: [0.005, 0.01]
     G4_zetas: [1.0, 4.0]
     G4_gammas: [1.0, -1]
     cutoff: 5.876798323827276
Device: cpu
Model: FullNN(
  (elementwise_models): ModuleDict(
    (Cu): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
    (O): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
    (C): MLP(
      (model_net): Sequential(
        (0): Linear(in_features=60, out_features=20, bias=True)
        (1): Tanh()
        (2): Linear(in_features=20, out_features=20, bias=True)
        (3): Tanh()
        (4): Linear(in_features=20, out_features=1, bias=True)
      )
    )
  )
)
Architecture:
   Input Layer - 60
   # of Hidden Layers - 2
   Nodes/Layer - 20
Loss Function: <class 'amptorch.model.CustomLoss'>
Force coefficient: 0.04
Optimizer: <class 'torch.optim.lbfgs.LBFGS'>
Learning Rate: 0.1
Batch Size: 200
Epochs: 200
Shuffle: False
Train Split (k-fold if int, fraction if float): 5

